{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Use Case: Summarize Dialogue\n",
    "\n",
    "Welcome to the practical side of this course. In this lab you will do the dialogue summarization task using generative AI. You will explore how the input text affects the output of the model, and perform prompt engineering to direct it towards the task you need. By comparing zero shot, one shot, and few shot inferences, you will take the first step towards prompt engineering and see how it can enhance the generative output of Large Language Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ 1 - Set up Kernel and Required Dependencies](#1)\n",
    "- [ 2 - Summarize Dialogue without Prompt Engineering](#2)\n",
    "- [ 3 - Summarize Dialogue with an Instruction Prompt](#3)\n",
    "  - [ 3.1 - Zero Shot Inference with an Instruction Prompt](#3.1)\n",
    "  - [ 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5](#3.2)\n",
    "- [ 4 - Summarize Dialogue with One Shot and Few Shot Inference](#4)\n",
    "  - [ 4.1 - One Shot Inference](#4.1)\n",
    "  - [ 4.2 - Few Shot Inference](#4.2)\n",
    "- [ 5 - Generative Configuration Parameters for Inference](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Set up Kernel and Required Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, check that the correct kernel is chosen.\n",
    "\n",
    "<img src=\"images/kernel_set_up.png\" width=\"300\"/>\n",
    "\n",
    "You can click on that (top right of the screen) to see and check the details of the image, kernel, and instance type.\n",
    "\n",
    "<img src=\"images/w1_kernel_and_instance_type.png\" width=\"600\"/>\n",
    "\n",
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik01MCw5NyBBNTAsNTAgMCAwIDEgNTMsMyBMNzk3LCAzIEw3OTcsOTcgTDUwLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPHRleHQgeD0iMTAwIiB5PSIzNCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5QbGVhc2UgbWFrZSBzdXJlIHRoYXQgeW91IGNob29zZTwvdGV4dD4gCiAgICA8dGV4dCB4PSIzMjAiIHk9IjM0IiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMTQiIGZpbGw9IiMzMzMzMzMiIGZvbnQtd2VpZ2h0PSJib2xkIj5tbC5tNS4yeGxhcmdlPC90ZXh0PgogICAgPHRleHQgeD0iNDE4IiB5PSIzNCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5pbnN0YW5jZSB0eXBlLjwvdGV4dD4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iNTYiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VG8gZmluZCB0aGF0IGluc3RhbmNlIHR5cGUsIHlvdSBtaWdodCBoYXZlIHRvIHNjcm9sbCBkb3duIHRvIHRoZSAiQWxsIEluc3RhbmNlcyIgc2VjdGlvbiBpbiB0aGUgZHJvcGRvd24uPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI3OCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5DaG9pY2Ugb2YgYW5vdGhlciBpbnN0YW5jZSB0eXBlIG1pZ2h0IGNhdXNlIHRyYWluaW5nIGZhaWx1cmUva2VybmVsIGhhbHQvYWNjb3VudCBkZWFjdGl2YXRpb24uPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert close\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now install the required packages to use PyTorch and Hugging Face transformers and datasets.\n",
    "\n",
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original hunks shape: (40405, 18)\n",
      "Shape of ['Python', 'Java', 'JavaScript', 'PHP', 'Ruby', 'Go', 'C', 'C#'] hunks: (26718, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_before</th>\n",
       "      <th>code_after</th>\n",
       "      <th>programming_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*********************************************...</td>\n",
       "      <td>*********************************************...</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>private boolean exractAndLoad(ArrayList&lt;St...</td>\n",
       "      <td>private boolean exractAndLoad(ArrayList&lt;St...</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n            // Fall back to extr...</td>\n",
       "      <td>\\n            // Fall back to extr...</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>return libName;\\n    }\\n\\n    private ...</td>\n",
       "      <td>return libName;\\n    }\\n\\n    private ...</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*  - http://www.gnu.org/copyleft/gpl.html\\n *...</td>\n",
       "      <td>*  - http://www.gnu.org/copyleft/gpl.html\\n *...</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         code_before   \n",
       "0   *********************************************...  \\\n",
       "1      private boolean exractAndLoad(ArrayList<St...   \n",
       "2              \\n            // Fall back to extr...   \n",
       "3          return libName;\\n    }\\n\\n    private ...   \n",
       "4   *  - http://www.gnu.org/copyleft/gpl.html\\n *...   \n",
       "\n",
       "                                          code_after programming_language  \n",
       "0   *********************************************...                 Java  \n",
       "1      private boolean exractAndLoad(ArrayList<St...                 Java  \n",
       "2              \\n            // Fall back to extr...                 Java  \n",
       "3          return libName;\\n    }\\n\\n    private ...                 Java  \n",
       "4   *  - http://www.gnu.org/copyleft/gpl.html\\n *...           JavaScript  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "import sqlite3\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# programming languages that CodeT5 supports\n",
    "languages = ['Python', 'Java', 'JavaScript', 'PHP', 'Ruby', 'Go', 'C', 'C#']\n",
    "# languages = ['Java']\n",
    "\n",
    "conn = sqlite3.connect('/Users/guru/research/FixMe/data/FixMe-v1.db')\n",
    "\n",
    "df_cve = pd.read_sql_query(\"SELECT * FROM cve;\", conn)\n",
    "df_repository = pd.read_sql_query(\"SELECT * FROM repository;\", conn)\n",
    "df_hunk = pd.read_sql_query(\"SELECT * FROM hunk_collection;\", conn)\n",
    "df_patch = pd.read_sql_query(\"SELECT * FROM patch_collection;\", conn)\n",
    "print(f'Original hunks shape: {df_hunk.shape}')\n",
    "\n",
    "df = df_hunk[df_hunk.programming_language.isin(languages)].reset_index(drop=True)\n",
    "df = df[['code_before', 'code_after', 'programming_language']]\n",
    "print(f'Shape of {languages} hunks: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file', 'hunk', 'hunk_patch', 'source', 'target', 'source_lines',\n",
       "       'target_lines', 'added_lines', 'removed_lines', 'code_before',\n",
       "       'code_after', 'source_start', 'source_length', 'target_start',\n",
       "       'target_length', 'section_header', 'hunk_length',\n",
       "       'programming_language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hunk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file', 'patch_info', 'programming_language', 'source_file',\n",
       "       'source_timestamp', 'target_file', 'target_timestamp', 'is_binary_file',\n",
       "       'url', 'message'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>hunk</th>\n",
       "      <th>hunk_patch</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_lines</th>\n",
       "      <th>target_lines</th>\n",
       "      <th>added_lines</th>\n",
       "      <th>removed_lines</th>\n",
       "      <th>code_before</th>\n",
       "      <th>code_after</th>\n",
       "      <th>source_start</th>\n",
       "      <th>source_length</th>\n",
       "      <th>target_start</th>\n",
       "      <th>target_length</th>\n",
       "      <th>section_header</th>\n",
       "      <th>hunk_length</th>\n",
       "      <th>programming_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>src/libraw_cxx.cpp</td>\n",
       "      <td>@@ -2336,14 +2336,15 @@ int LibRaw::subtract_b...</td>\n",
       "      <td>@@ -2336,14 +2336,15 @@ int LibRaw::subtract_b...</td>\n",
       "      <td>[' #define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\\n'...</td>\n",
       "      <td>[' #define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\\n'...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['\\t\\t\\tint dmax = 0;\\n', '\\t\\t\\tfor(i=0; i&lt; s...</td>\n",
       "      <td>['\\n', '            for(i=0; i&lt; size*4; i++)\\n...</td>\n",
       "      <td>#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\\n#def...</td>\n",
       "      <td>#define MAX(a,b) ((a) &gt; (b) ? (a) : (b))\\n#def...</td>\n",
       "      <td>2336</td>\n",
       "      <td>14</td>\n",
       "      <td>2336</td>\n",
       "      <td>15</td>\n",
       "      <td>int LibRaw::subtract_black()</td>\n",
       "      <td>18</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>src/libraw_cxx.cpp</td>\n",
       "      <td>@@ -2359,9 +2360,10 @@ int LibRaw::subtract_bl...</td>\n",
       "      <td>@@ -2359,9 +2360,10 @@ int LibRaw::subtract_bl...</td>\n",
       "      <td>['           // only calculate channel maximum...</td>\n",
       "      <td>['           // only calculate channel maximum...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['\\t\\t  int dmax = 0;\\n', '            if(dmax...</td>\n",
       "      <td>['          C.data_maximum = 0;\\n', '         ...</td>\n",
       "      <td>// only calculate channel maximum;\\n...</td>\n",
       "      <td>// only calculate channel maximum;\\n...</td>\n",
       "      <td>2359</td>\n",
       "      <td>9</td>\n",
       "      <td>2360</td>\n",
       "      <td>10</td>\n",
       "      <td>int LibRaw::subtract_black()</td>\n",
       "      <td>12</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>src/libraw_cxx.cpp</td>\n",
       "      <td>@@ -2421,8 +2423,10 @@ void LibRaw::exp_bef(fl...</td>\n",
       "      <td>@@ -2421,8 +2423,10 @@ void LibRaw::exp_bef(fl...</td>\n",
       "      <td>['             imgdata.image[i][3] = lut[imgda...</td>\n",
       "      <td>['             imgdata.image[i][3] = lut[imgda...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['\\tif(C.data_maximum &lt;=TBLN)\\n', '\\t\\tC.data_...</td>\n",
       "      <td>['    C.data_maximum = lut[C.data_maximum];\\n'...</td>\n",
       "      <td>imgdata.image[i][3] = lut[imgdata....</td>\n",
       "      <td>imgdata.image[i][3] = lut[imgdata....</td>\n",
       "      <td>2421</td>\n",
       "      <td>8</td>\n",
       "      <td>2423</td>\n",
       "      <td>10</td>\n",
       "      <td>void LibRaw::exp_bef(float shift, float smooth)</td>\n",
       "      <td>12</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>src/libraw_cxx.cpp</td>\n",
       "      <td>@@ -2530,7 +2534,7 @@ int LibRaw::dcraw_proces...</td>\n",
       "      <td>@@ -2530,7 +2534,7 @@ int LibRaw::dcraw_proces...</td>\n",
       "      <td>[' \\n', '         raw2image_ex(subtract_inline...</td>\n",
       "      <td>[' \\n', '         raw2image_ex(subtract_inline...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['\\t\\tint save_4color = O.four_color_rgb;\\n']</td>\n",
       "      <td>['        int save_4color = O.four_color_rgb;\\n']</td>\n",
       "      <td>\\n        raw2image_ex(subtract_inline); // al...</td>\n",
       "      <td>\\n        raw2image_ex(subtract_inline); // al...</td>\n",
       "      <td>2530</td>\n",
       "      <td>7</td>\n",
       "      <td>2534</td>\n",
       "      <td>7</td>\n",
       "      <td>int LibRaw::dcraw_process(void)</td>\n",
       "      <td>8</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hawtjni-runtime/src/main/java/org/fusesource/h...</td>\n",
       "      <td>@@ -9,13 +9,11 @@\\n  *************************...</td>\n",
       "      <td>@@ -9,13 +9,11 @@\\n  *************************...</td>\n",
       "      <td>['  ******************************************...</td>\n",
       "      <td>['  ******************************************...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 9...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 9...</td>\n",
       "      <td>['import java.io.*;\\n', 'import java.util.Rand...</td>\n",
       "      <td>['import java.io.File;\\n', 'import java.io.Fil...</td>\n",
       "      <td>*********************************************...</td>\n",
       "      <td>*********************************************...</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file   \n",
       "0                                 src/libraw_cxx.cpp  \\\n",
       "1                                 src/libraw_cxx.cpp   \n",
       "2                                 src/libraw_cxx.cpp   \n",
       "3                                 src/libraw_cxx.cpp   \n",
       "4  hawtjni-runtime/src/main/java/org/fusesource/h...   \n",
       "\n",
       "                                                hunk   \n",
       "0  @@ -2336,14 +2336,15 @@ int LibRaw::subtract_b...  \\\n",
       "1  @@ -2359,9 +2360,10 @@ int LibRaw::subtract_bl...   \n",
       "2  @@ -2421,8 +2423,10 @@ void LibRaw::exp_bef(fl...   \n",
       "3  @@ -2530,7 +2534,7 @@ int LibRaw::dcraw_proces...   \n",
       "4  @@ -9,13 +9,11 @@\\n  *************************...   \n",
       "\n",
       "                                          hunk_patch   \n",
       "0  @@ -2336,14 +2336,15 @@ int LibRaw::subtract_b...  \\\n",
       "1  @@ -2359,9 +2360,10 @@ int LibRaw::subtract_bl...   \n",
       "2  @@ -2421,8 +2423,10 @@ void LibRaw::exp_bef(fl...   \n",
       "3  @@ -2530,7 +2534,7 @@ int LibRaw::dcraw_proces...   \n",
       "4  @@ -9,13 +9,11 @@\\n  *************************...   \n",
       "\n",
       "                                              source   \n",
       "0  [' #define MAX(a,b) ((a) > (b) ? (a) : (b))\\n'...  \\\n",
       "1  ['           // only calculate channel maximum...   \n",
       "2  ['             imgdata.image[i][3] = lut[imgda...   \n",
       "3  [' \\n', '         raw2image_ex(subtract_inline...   \n",
       "4  ['  ******************************************...   \n",
       "\n",
       "                                              target   \n",
       "0  [' #define MAX(a,b) ((a) > (b) ? (a) : (b))\\n'...  \\\n",
       "1  ['           // only calculate channel maximum...   \n",
       "2  ['             imgdata.image[i][3] = lut[imgda...   \n",
       "3  [' \\n', '         raw2image_ex(subtract_inline...   \n",
       "4  ['  ******************************************...   \n",
       "\n",
       "                                        source_lines   \n",
       "0  <bound method Hunk.source_lines of <Hunk: @@ 2...  \\\n",
       "1  <bound method Hunk.source_lines of <Hunk: @@ 2...   \n",
       "2  <bound method Hunk.source_lines of <Hunk: @@ 2...   \n",
       "3  <bound method Hunk.source_lines of <Hunk: @@ 2...   \n",
       "4  <bound method Hunk.source_lines of <Hunk: @@ 9...   \n",
       "\n",
       "                                        target_lines   \n",
       "0  <bound method Hunk.target_lines of <Hunk: @@ 2...  \\\n",
       "1  <bound method Hunk.target_lines of <Hunk: @@ 2...   \n",
       "2  <bound method Hunk.target_lines of <Hunk: @@ 2...   \n",
       "3  <bound method Hunk.target_lines of <Hunk: @@ 2...   \n",
       "4  <bound method Hunk.target_lines of <Hunk: @@ 9...   \n",
       "\n",
       "                                         added_lines   \n",
       "0  ['\\t\\t\\tint dmax = 0;\\n', '\\t\\t\\tfor(i=0; i< s...  \\\n",
       "1  ['\\t\\t  int dmax = 0;\\n', '            if(dmax...   \n",
       "2  ['\\tif(C.data_maximum <=TBLN)\\n', '\\t\\tC.data_...   \n",
       "3      ['\\t\\tint save_4color = O.four_color_rgb;\\n']   \n",
       "4  ['import java.io.*;\\n', 'import java.util.Rand...   \n",
       "\n",
       "                                       removed_lines   \n",
       "0  ['\\n', '            for(i=0; i< size*4; i++)\\n...  \\\n",
       "1  ['          C.data_maximum = 0;\\n', '         ...   \n",
       "2  ['    C.data_maximum = lut[C.data_maximum];\\n'...   \n",
       "3  ['        int save_4color = O.four_color_rgb;\\n']   \n",
       "4  ['import java.io.File;\\n', 'import java.io.Fil...   \n",
       "\n",
       "                                         code_before   \n",
       "0  #define MAX(a,b) ((a) > (b) ? (a) : (b))\\n#def...  \\\n",
       "1            // only calculate channel maximum;\\n...   \n",
       "2              imgdata.image[i][3] = lut[imgdata....   \n",
       "3  \\n        raw2image_ex(subtract_inline); // al...   \n",
       "4   *********************************************...   \n",
       "\n",
       "                                          code_after source_start   \n",
       "0  #define MAX(a,b) ((a) > (b) ? (a) : (b))\\n#def...         2336  \\\n",
       "1            // only calculate channel maximum;\\n...         2359   \n",
       "2              imgdata.image[i][3] = lut[imgdata....         2421   \n",
       "3  \\n        raw2image_ex(subtract_inline); // al...         2530   \n",
       "4   *********************************************...            9   \n",
       "\n",
       "  source_length target_start target_length   \n",
       "0            14         2336            15  \\\n",
       "1             9         2360            10   \n",
       "2             8         2423            10   \n",
       "3             7         2534             7   \n",
       "4            13            9            11   \n",
       "\n",
       "                                    section_header hunk_length   \n",
       "0                     int LibRaw::subtract_black()          18  \\\n",
       "1                     int LibRaw::subtract_black()          12   \n",
       "2  void LibRaw::exp_bef(float shift, float smooth)          12   \n",
       "3                  int LibRaw::dcraw_process(void)           8   \n",
       "4                                                           15   \n",
       "\n",
       "  programming_language  \n",
       "0                  C++  \n",
       "1                  C++  \n",
       "2                  C++  \n",
       "3                  C++  \n",
       "4                 Java  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of filtered patch data: (771, 10)\n",
      "Shape of filtered hunk data: (2840, 18)\n",
      "Shape of filtered hunks (pl-list): (1880, 18)\n"
     ]
    }
   ],
   "source": [
    "def filter_patches(df_patch, max_hunks_per_file=2):\n",
    "    \"\"\"Filter URLs with counts less than max_hunks_per_file\"\"\"\n",
    "    # Calculate value counts of 'url' column\n",
    "    url_counts = df_patch['url'].value_counts()\n",
    "    urls_less_than_two = url_counts[url_counts <= max_hunks_per_file].index.tolist()\n",
    "    df_filter = df_patch[df_patch.url.isin(urls_less_than_two)]\n",
    "\n",
    "    print(f'Shape of filtered patch data: {df_filter.shape}')\n",
    "    return df_filter\n",
    "\n",
    "def filter_hunks(df_hunk, df_patch):\n",
    "    \"\"\"Filter hunks that are not in the filtered patches\"\"\"\n",
    "    df_hunk = df_hunk[df_hunk.file.isin(df_patch.file)]\n",
    "    print(f'Shape of filtered hunk data: {df_hunk.shape}')\n",
    "    return df_hunk\n",
    "\n",
    "max_hunks_per_file = 1\n",
    "df_patch_filter = filter_patches(df_patch, max_hunks_per_file)\n",
    "df_hunk_filter = filter_hunks(df_hunk, df_patch_filter)\n",
    "\n",
    "df_hunk_filter = df_hunk_filter[df_hunk_filter.programming_language.isin(languages)].reset_index(drop=True)\n",
    "print(f'Shape of filtered hunks (pl-list): {df_hunk_filter.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>hunk</th>\n",
       "      <th>hunk_patch</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_lines</th>\n",
       "      <th>target_lines</th>\n",
       "      <th>added_lines</th>\n",
       "      <th>removed_lines</th>\n",
       "      <th>code_before</th>\n",
       "      <th>code_after</th>\n",
       "      <th>source_start</th>\n",
       "      <th>source_length</th>\n",
       "      <th>target_start</th>\n",
       "      <th>target_length</th>\n",
       "      <th>section_header</th>\n",
       "      <th>hunk_length</th>\n",
       "      <th>programming_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hawtjni-runtime/src/main/java/org/fusesource/h...</td>\n",
       "      <td>@@ -9,13 +9,11 @@\\n  *************************...</td>\n",
       "      <td>@@ -9,13 +9,11 @@\\n  *************************...</td>\n",
       "      <td>['  ******************************************...</td>\n",
       "      <td>['  ******************************************...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 9...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 9...</td>\n",
       "      <td>['import java.io.*;\\n', 'import java.util.Rand...</td>\n",
       "      <td>['import java.io.File;\\n', 'import java.io.Fil...</td>\n",
       "      <td>*********************************************...</td>\n",
       "      <td>*********************************************...</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hawtjni-runtime/src/main/java/org/fusesource/h...</td>\n",
       "      <td>@@ -206,16 +204,19 @@ final public String getL...</td>\n",
       "      <td>@@ -206,16 +204,19 @@ final public String getL...</td>\n",
       "      <td>['     private boolean exractAndLoad(ArrayList...</td>\n",
       "      <td>['     private boolean exractAndLoad(ArrayList...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['\\n', '            String []libNameParts = ma...</td>\n",
       "      <td>['            \\n', '            \\n', '        ...</td>\n",
       "      <td>private boolean exractAndLoad(ArrayList&lt;St...</td>\n",
       "      <td>private boolean exractAndLoad(ArrayList&lt;St...</td>\n",
       "      <td>206</td>\n",
       "      <td>16</td>\n",
       "      <td>204</td>\n",
       "      <td>19</td>\n",
       "      <td>final public String getLibraryFileName() {</td>\n",
       "      <td>23</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hawtjni-runtime/src/main/java/org/fusesource/h...</td>\n",
       "      <td>@@ -224,8 +225,8 @@ private boolean exractAndL...</td>\n",
       "      <td>@@ -224,8 +225,8 @@ private boolean exractAndL...</td>\n",
       "      <td>['             \\n', '             // Fall back...</td>\n",
       "      <td>['             \\n', '             // Fall back...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['            File target = extract(errors, re...</td>\n",
       "      <td>['            File target = file(customPath, m...</td>\n",
       "      <td>\\n            // Fall back to extr...</td>\n",
       "      <td>\\n            // Fall back to extr...</td>\n",
       "      <td>224</td>\n",
       "      <td>8</td>\n",
       "      <td>225</td>\n",
       "      <td>8</td>\n",
       "      <td>private boolean exractAndLoad(ArrayList&lt;String...</td>\n",
       "      <td>10</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hawtjni-runtime/src/main/java/org/fusesource/h...</td>\n",
       "      <td>@@ -259,67 +260,45 @@ private String map(Strin...</td>\n",
       "      <td>@@ -259,67 +260,45 @@ private String map(Strin...</td>\n",
       "      <td>['         return libName;\\n', '     }\\n', ' \\...</td>\n",
       "      <td>['         return libName;\\n', '     }\\n', ' \\...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['    private File extract(ArrayList&lt;String&gt; e...</td>\n",
       "      <td>['    private boolean extract(ArrayList&lt;String...</td>\n",
       "      <td>return libName;\\n    }\\n\\n    private ...</td>\n",
       "      <td>return libName;\\n    }\\n\\n    private ...</td>\n",
       "      <td>259</td>\n",
       "      <td>67</td>\n",
       "      <td>260</td>\n",
       "      <td>45</td>\n",
       "      <td>private String map(String libName) {</td>\n",
       "      <td>85</td>\n",
       "      <td>Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drivers/usb/chipidea/host.c</td>\n",
       "      <td>@@ -70,6 +70,9 @@ static int host_start(struct...</td>\n",
       "      <td>@@ -70,6 +70,9 @@ static int host_start(struct...</td>\n",
       "      <td>[' \\telse\\n', ' \\t\\tci-&gt;hcd = hcd;\\n', ' \\n', ...</td>\n",
       "      <td>[' \\telse\\n', ' \\t\\tci-&gt;hcd = hcd;\\n', ' \\n', ...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 7...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 7...</td>\n",
       "      <td>['\\tif (ci-&gt;platdata-&gt;flags &amp; CI13XXX_DISABLE_...</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\telse\\n\\t\\tci-&gt;hcd = hcd;\\n\\n\\treturn ret;\\n}...</td>\n",
       "      <td>\\telse\\n\\t\\tci-&gt;hcd = hcd;\\n\\n\\tif (ci-&gt;platda...</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>static int host_start(struct ci13xxx *ci)</td>\n",
       "      <td>9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>src/lib/filters.c</td>\n",
       "      <td>@@ -1205,13 +1205,19 @@ void print_abinary(cha...</td>\n",
       "      <td>@@ -1205,13 +1205,19 @@ void print_abinary(cha...</td>\n",
       "      <td>[' \\t\\t\\t}\\n', ' \\t\\t}\\n', ' \\t} else if (filt...</td>\n",
       "      <td>[' \\t\\t\\t}\\n', ' \\t\\t}\\n', ' \\t} else if (filt...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 1...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 1...</td>\n",
       "      <td>['\\t\\tsize_t count, masklen;\\n', '\\n', '\\t\\tma...</td>\n",
       "      <td>['\\t\\tint count;\\n', '\\t\\tfor (count = 0; coun...</td>\n",
       "      <td>\\t\\t\\t}\\n\\t\\t}\\n\\t} else if (filter-&gt;type == R...</td>\n",
       "      <td>\\t\\t\\t}\\n\\t\\t}\\n\\t} else if (filter-&gt;type == R...</td>\n",
       "      <td>1205</td>\n",
       "      <td>13</td>\n",
       "      <td>1205</td>\n",
       "      <td>19</td>\n",
       "      <td>void print_abinary(char *out, size_t outlen, u...</td>\n",
       "      <td>21</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>src/lib/filters.c</td>\n",
       "      <td>@@ -1222,7 +1228,7 @@ void print_abinary(char ...</td>\n",
       "      <td>@@ -1222,7 +1228,7 @@ void print_abinary(char ...</td>\n",
       "      <td>[' \\t\\toutlen--;\\n', ' \\n', ' \\t\\t/* show the ...</td>\n",
       "      <td>[' \\t\\toutlen--;\\n', ' \\n', ' \\t\\t/* show the ...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 1...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 1...</td>\n",
       "      <td>['\\t\\tfor (count = 0; count &lt; masklen; count++...</td>\n",
       "      <td>['\\t\\tfor (count = 0; count &lt; ntohs(filter-&gt;u....</td>\n",
       "      <td>\\t\\toutlen--;\\n\\n\\t\\t/* show the value */\\n\\t\\...</td>\n",
       "      <td>\\t\\toutlen--;\\n\\n\\t\\t/* show the value */\\n\\t\\...</td>\n",
       "      <td>1222</td>\n",
       "      <td>7</td>\n",
       "      <td>1228</td>\n",
       "      <td>7</td>\n",
       "      <td>void print_abinary(char *out, size_t outlen, u...</td>\n",
       "      <td>8</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>channels/drive/client/drive_main.c</td>\n",
       "      <td>@@ -629,6 +629,9 @@ static UINT drive_process_...</td>\n",
       "      <td>@@ -629,6 +629,9 @@ static UINT drive_process_...</td>\n",
       "      <td>[' \\tStream_Read_UINT32(irp-&gt;input, PathLength...</td>\n",
       "      <td>[' \\tStream_Read_UINT32(irp-&gt;input, PathLength...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 6...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 6...</td>\n",
       "      <td>['\\tif (!Stream_CheckAndLogRequiredLength(TAG,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\tStream_Read_UINT32(irp-&gt;input, PathLength);\\...</td>\n",
       "      <td>\\tStream_Read_UINT32(irp-&gt;input, PathLength);\\...</td>\n",
       "      <td>629</td>\n",
       "      <td>6</td>\n",
       "      <td>629</td>\n",
       "      <td>9</td>\n",
       "      <td>static UINT drive_process_irp_query_directory(...</td>\n",
       "      <td>9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>zerver/middleware.py</td>\n",
       "      <td>@@ -25,6 +25,7 @@\\n from django.shortcuts impo...</td>\n",
       "      <td>@@ -25,6 +25,7 @@\\n from django.shortcuts impo...</td>\n",
       "      <td>[' from django.shortcuts import render\\n', ' f...</td>\n",
       "      <td>[' from django.shortcuts import render\\n', ' f...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 2...</td>\n",
       "      <td>['from django.utils.crypto import constant_tim...</td>\n",
       "      <td>[]</td>\n",
       "      <td>from django.shortcuts import render\\nfrom djan...</td>\n",
       "      <td>from django.shortcuts import render\\nfrom djan...</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>zerver/middleware.py</td>\n",
       "      <td>@@ -704,7 +705,10 @@ def validate_scim_bearer_...</td>\n",
       "      <td>@@ -704,7 +705,10 @@ def validate_scim_bearer_...</td>\n",
       "      <td>['     assert valid_bearer_token\\n', '     ass...</td>\n",
       "      <td>['     assert valid_bearer_token\\n', '     ass...</td>\n",
       "      <td>&lt;bound method Hunk.source_lines of &lt;Hunk: @@ 7...</td>\n",
       "      <td>&lt;bound method Hunk.target_lines of &lt;Hunk: @@ 7...</td>\n",
       "      <td>['    authorization = request.headers.get(\"Aut...</td>\n",
       "      <td>['    if request.headers.get(\"Authorization\") ...</td>\n",
       "      <td>assert valid_bearer_token\\n    assert scim...</td>\n",
       "      <td>assert valid_bearer_token\\n    assert scim...</td>\n",
       "      <td>704</td>\n",
       "      <td>7</td>\n",
       "      <td>705</td>\n",
       "      <td>10</td>\n",
       "      <td>def validate_scim_bearer_token(request: HttpRe...</td>\n",
       "      <td>11</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1880 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file   \n",
       "0     hawtjni-runtime/src/main/java/org/fusesource/h...  \\\n",
       "1     hawtjni-runtime/src/main/java/org/fusesource/h...   \n",
       "2     hawtjni-runtime/src/main/java/org/fusesource/h...   \n",
       "3     hawtjni-runtime/src/main/java/org/fusesource/h...   \n",
       "4                           drivers/usb/chipidea/host.c   \n",
       "...                                                 ...   \n",
       "1875                                  src/lib/filters.c   \n",
       "1876                                  src/lib/filters.c   \n",
       "1877                 channels/drive/client/drive_main.c   \n",
       "1878                               zerver/middleware.py   \n",
       "1879                               zerver/middleware.py   \n",
       "\n",
       "                                                   hunk   \n",
       "0     @@ -9,13 +9,11 @@\\n  *************************...  \\\n",
       "1     @@ -206,16 +204,19 @@ final public String getL...   \n",
       "2     @@ -224,8 +225,8 @@ private boolean exractAndL...   \n",
       "3     @@ -259,67 +260,45 @@ private String map(Strin...   \n",
       "4     @@ -70,6 +70,9 @@ static int host_start(struct...   \n",
       "...                                                 ...   \n",
       "1875  @@ -1205,13 +1205,19 @@ void print_abinary(cha...   \n",
       "1876  @@ -1222,7 +1228,7 @@ void print_abinary(char ...   \n",
       "1877  @@ -629,6 +629,9 @@ static UINT drive_process_...   \n",
       "1878  @@ -25,6 +25,7 @@\\n from django.shortcuts impo...   \n",
       "1879  @@ -704,7 +705,10 @@ def validate_scim_bearer_...   \n",
       "\n",
       "                                             hunk_patch   \n",
       "0     @@ -9,13 +9,11 @@\\n  *************************...  \\\n",
       "1     @@ -206,16 +204,19 @@ final public String getL...   \n",
       "2     @@ -224,8 +225,8 @@ private boolean exractAndL...   \n",
       "3     @@ -259,67 +260,45 @@ private String map(Strin...   \n",
       "4     @@ -70,6 +70,9 @@ static int host_start(struct...   \n",
       "...                                                 ...   \n",
       "1875  @@ -1205,13 +1205,19 @@ void print_abinary(cha...   \n",
       "1876  @@ -1222,7 +1228,7 @@ void print_abinary(char ...   \n",
       "1877  @@ -629,6 +629,9 @@ static UINT drive_process_...   \n",
       "1878  @@ -25,6 +25,7 @@\\n from django.shortcuts impo...   \n",
       "1879  @@ -704,7 +705,10 @@ def validate_scim_bearer_...   \n",
       "\n",
       "                                                 source   \n",
       "0     ['  ******************************************...  \\\n",
       "1     ['     private boolean exractAndLoad(ArrayList...   \n",
       "2     ['             \\n', '             // Fall back...   \n",
       "3     ['         return libName;\\n', '     }\\n', ' \\...   \n",
       "4     [' \\telse\\n', ' \\t\\tci->hcd = hcd;\\n', ' \\n', ...   \n",
       "...                                                 ...   \n",
       "1875  [' \\t\\t\\t}\\n', ' \\t\\t}\\n', ' \\t} else if (filt...   \n",
       "1876  [' \\t\\toutlen--;\\n', ' \\n', ' \\t\\t/* show the ...   \n",
       "1877  [' \\tStream_Read_UINT32(irp->input, PathLength...   \n",
       "1878  [' from django.shortcuts import render\\n', ' f...   \n",
       "1879  ['     assert valid_bearer_token\\n', '     ass...   \n",
       "\n",
       "                                                 target   \n",
       "0     ['  ******************************************...  \\\n",
       "1     ['     private boolean exractAndLoad(ArrayList...   \n",
       "2     ['             \\n', '             // Fall back...   \n",
       "3     ['         return libName;\\n', '     }\\n', ' \\...   \n",
       "4     [' \\telse\\n', ' \\t\\tci->hcd = hcd;\\n', ' \\n', ...   \n",
       "...                                                 ...   \n",
       "1875  [' \\t\\t\\t}\\n', ' \\t\\t}\\n', ' \\t} else if (filt...   \n",
       "1876  [' \\t\\toutlen--;\\n', ' \\n', ' \\t\\t/* show the ...   \n",
       "1877  [' \\tStream_Read_UINT32(irp->input, PathLength...   \n",
       "1878  [' from django.shortcuts import render\\n', ' f...   \n",
       "1879  ['     assert valid_bearer_token\\n', '     ass...   \n",
       "\n",
       "                                           source_lines   \n",
       "0     <bound method Hunk.source_lines of <Hunk: @@ 9...  \\\n",
       "1     <bound method Hunk.source_lines of <Hunk: @@ 2...   \n",
       "2     <bound method Hunk.source_lines of <Hunk: @@ 2...   \n",
       "3     <bound method Hunk.source_lines of <Hunk: @@ 2...   \n",
       "4     <bound method Hunk.source_lines of <Hunk: @@ 7...   \n",
       "...                                                 ...   \n",
       "1875  <bound method Hunk.source_lines of <Hunk: @@ 1...   \n",
       "1876  <bound method Hunk.source_lines of <Hunk: @@ 1...   \n",
       "1877  <bound method Hunk.source_lines of <Hunk: @@ 6...   \n",
       "1878  <bound method Hunk.source_lines of <Hunk: @@ 2...   \n",
       "1879  <bound method Hunk.source_lines of <Hunk: @@ 7...   \n",
       "\n",
       "                                           target_lines   \n",
       "0     <bound method Hunk.target_lines of <Hunk: @@ 9...  \\\n",
       "1     <bound method Hunk.target_lines of <Hunk: @@ 2...   \n",
       "2     <bound method Hunk.target_lines of <Hunk: @@ 2...   \n",
       "3     <bound method Hunk.target_lines of <Hunk: @@ 2...   \n",
       "4     <bound method Hunk.target_lines of <Hunk: @@ 7...   \n",
       "...                                                 ...   \n",
       "1875  <bound method Hunk.target_lines of <Hunk: @@ 1...   \n",
       "1876  <bound method Hunk.target_lines of <Hunk: @@ 1...   \n",
       "1877  <bound method Hunk.target_lines of <Hunk: @@ 6...   \n",
       "1878  <bound method Hunk.target_lines of <Hunk: @@ 2...   \n",
       "1879  <bound method Hunk.target_lines of <Hunk: @@ 7...   \n",
       "\n",
       "                                            added_lines   \n",
       "0     ['import java.io.*;\\n', 'import java.util.Rand...  \\\n",
       "1     ['\\n', '            String []libNameParts = ma...   \n",
       "2     ['            File target = extract(errors, re...   \n",
       "3     ['    private File extract(ArrayList<String> e...   \n",
       "4     ['\\tif (ci->platdata->flags & CI13XXX_DISABLE_...   \n",
       "...                                                 ...   \n",
       "1875  ['\\t\\tsize_t count, masklen;\\n', '\\n', '\\t\\tma...   \n",
       "1876  ['\\t\\tfor (count = 0; count < masklen; count++...   \n",
       "1877  ['\\tif (!Stream_CheckAndLogRequiredLength(TAG,...   \n",
       "1878  ['from django.utils.crypto import constant_tim...   \n",
       "1879  ['    authorization = request.headers.get(\"Aut...   \n",
       "\n",
       "                                          removed_lines   \n",
       "0     ['import java.io.File;\\n', 'import java.io.Fil...  \\\n",
       "1     ['            \\n', '            \\n', '        ...   \n",
       "2     ['            File target = file(customPath, m...   \n",
       "3     ['    private boolean extract(ArrayList<String...   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "1875  ['\\t\\tint count;\\n', '\\t\\tfor (count = 0; coun...   \n",
       "1876  ['\\t\\tfor (count = 0; count < ntohs(filter->u....   \n",
       "1877                                                 []   \n",
       "1878                                                 []   \n",
       "1879  ['    if request.headers.get(\"Authorization\") ...   \n",
       "\n",
       "                                            code_before   \n",
       "0      *********************************************...  \\\n",
       "1         private boolean exractAndLoad(ArrayList<St...   \n",
       "2                 \\n            // Fall back to extr...   \n",
       "3             return libName;\\n    }\\n\\n    private ...   \n",
       "4     \\telse\\n\\t\\tci->hcd = hcd;\\n\\n\\treturn ret;\\n}...   \n",
       "...                                                 ...   \n",
       "1875  \\t\\t\\t}\\n\\t\\t}\\n\\t} else if (filter->type == R...   \n",
       "1876  \\t\\toutlen--;\\n\\n\\t\\t/* show the value */\\n\\t\\...   \n",
       "1877  \\tStream_Read_UINT32(irp->input, PathLength);\\...   \n",
       "1878  from django.shortcuts import render\\nfrom djan...   \n",
       "1879      assert valid_bearer_token\\n    assert scim...   \n",
       "\n",
       "                                             code_after source_start   \n",
       "0      *********************************************...            9  \\\n",
       "1         private boolean exractAndLoad(ArrayList<St...          206   \n",
       "2                 \\n            // Fall back to extr...          224   \n",
       "3             return libName;\\n    }\\n\\n    private ...          259   \n",
       "4     \\telse\\n\\t\\tci->hcd = hcd;\\n\\n\\tif (ci->platda...           70   \n",
       "...                                                 ...          ...   \n",
       "1875  \\t\\t\\t}\\n\\t\\t}\\n\\t} else if (filter->type == R...         1205   \n",
       "1876  \\t\\toutlen--;\\n\\n\\t\\t/* show the value */\\n\\t\\...         1222   \n",
       "1877  \\tStream_Read_UINT32(irp->input, PathLength);\\...          629   \n",
       "1878  from django.shortcuts import render\\nfrom djan...           25   \n",
       "1879      assert valid_bearer_token\\n    assert scim...          704   \n",
       "\n",
       "     source_length target_start target_length   \n",
       "0               13            9            11  \\\n",
       "1               16          204            19   \n",
       "2                8          225             8   \n",
       "3               67          260            45   \n",
       "4                6           70             9   \n",
       "...            ...          ...           ...   \n",
       "1875            13         1205            19   \n",
       "1876             7         1228             7   \n",
       "1877             6          629             9   \n",
       "1878             6           25             7   \n",
       "1879             7          705            10   \n",
       "\n",
       "                                         section_header hunk_length   \n",
       "0                                                                15  \\\n",
       "1            final public String getLibraryFileName() {          23   \n",
       "2     private boolean exractAndLoad(ArrayList<String...          10   \n",
       "3                  private String map(String libName) {          85   \n",
       "4             static int host_start(struct ci13xxx *ci)           9   \n",
       "...                                                 ...         ...   \n",
       "1875  void print_abinary(char *out, size_t outlen, u...          21   \n",
       "1876  void print_abinary(char *out, size_t outlen, u...           8   \n",
       "1877  static UINT drive_process_irp_query_directory(...           9   \n",
       "1878                                                              7   \n",
       "1879  def validate_scim_bearer_token(request: HttpRe...          11   \n",
       "\n",
       "     programming_language  \n",
       "0                    Java  \n",
       "1                    Java  \n",
       "2                    Java  \n",
       "3                    Java  \n",
       "4                       C  \n",
       "...                   ...  \n",
       "1875                    C  \n",
       "1876                    C  \n",
       "1877                    C  \n",
       "1878               Python  \n",
       "1879               Python  \n",
       "\n",
       "[1880 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hunk_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original code:\n",
      "\n",
      "\treturn 0;\n",
      "\n",
      "unmap_pages:\n",
      "\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn);\n",
      "\treturn r;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Modified code:\n",
      "    \n",
      "    \treturn 0;\n",
      "\n",
      "unmap_pages:\n",
      "\tkvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);\n",
      "\treturn r;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "    What is the modification related in the original code?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "index = 789\n",
    "prompt = \"\"\"\n",
    "Original code:\n",
    "\n",
    "\"\"\" + df_hunk_filter.code_before[index] + \"\"\"\n",
    "\n",
    "Modified code:\n",
    "    \n",
    "    \"\"\" + df_hunk_filter.code_after[index] + \"\"\"\n",
    "\n",
    "    What is the modification related in the original code?\n",
    "    \"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Original Code:\n",
      "raw2image_ex(subtract_inline); // allocate imgdata.image and copy data! int save_4color = O.four_color_rgb; if (IO.zero_is_bad) {\n",
      "\n",
      "Formatted New Code:\n",
      "raw2image_ex(subtract_inline); // allocate imgdata.image and copy data! int save_4color = O.four_color_rgb; if (IO.zero_is_bad) {\n",
      "Match: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def replace_whitespaces_N21(code):\n",
    "    # Use regular expression to replace multiple whitespaces with a single space\n",
    "    return re.sub(r'\\s+', ' ', code).strip()\n",
    "\n",
    "\n",
    "# Replace multiple whitespaces with a single space\n",
    "formatted_original_code = replace_whitespaces_N21(original_code)\n",
    "formatted_new_code = replace_whitespaces_N21(new_code)\n",
    "\n",
    "match = formatted_original_code == formatted_new_code\n",
    "print(f'Match: {match}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: ['cve', 'repository', 'hunk_collection', 'patch_collection']\n"
     ]
    }
   ],
   "source": [
    "def show_tables(conn):\n",
    "    # List all tables in the database\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    tables = pd.read_sql_query(query, conn)\n",
    "    print(\"Tables in the database:\", tables['name'].tolist())\n",
    "\n",
    "show_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original patches shape: (722, 10)\n"
     ]
    }
   ],
   "source": [
    "mask = df_patch.message.value_counts()\n",
    "mask = mask[mask <= 1].index\n",
    "df_patch_1 = df_patch[df_patch.message.isin(mask)].reset_index(drop=True)\n",
    "print(f'Original patches shape: {df_patch_1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       CVE-2013-2873\n",
       "1       CVE-2013-2080\n",
       "2       CVE-2013-2126\n",
       "3       CVE-2013-2204\n",
       "4       CVE-2013-2634\n",
       "            ...      \n",
       "1981    CVE-2005-4798\n",
       "1982    CVE-2005-4881\n",
       "1983    CVE-2005-4635\n",
       "1984    CVE-2005-1041\n",
       "1985    CVE-2005-1767\n",
       "Name: cveId, Length: 1986, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_repository.cveId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>patch_info</th>\n",
       "      <th>programming_language</th>\n",
       "      <th>source_file</th>\n",
       "      <th>source_timestamp</th>\n",
       "      <th>target_file</th>\n",
       "      <th>target_timestamp</th>\n",
       "      <th>is_binary_file</th>\n",
       "      <th>url</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>src/libraw_cxx.cpp</td>\n",
       "      <td>diff --git a/src/libraw_cxx.cpp b/src/libraw_c...</td>\n",
       "      <td>C++</td>\n",
       "      <td>a/src/libraw_cxx.cpp</td>\n",
       "      <td>None</td>\n",
       "      <td>b/src/libraw_cxx.cpp</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/LibRaw/LibRaw/commit/2f912f...</td>\n",
       "      <td>[PATCH] fixed wrong data_maximum calcluation; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hawtjni-runtime/src/main/java/org/fusesource/h...</td>\n",
       "      <td>diff --git a/hawtjni-runtime/src/main/java/org...</td>\n",
       "      <td>Java</td>\n",
       "      <td>a/hawtjni-runtime/src/main/java/org/fusesource...</td>\n",
       "      <td>None</td>\n",
       "      <td>b/hawtjni-runtime/src/main/java/org/fusesource...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/fusesource/hawtjni/commit/9...</td>\n",
       "      <td>[PATCH] Simplify shared lib extraction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drivers/usb/chipidea/host.c</td>\n",
       "      <td>diff --git a/drivers/usb/chipidea/host.c b/dri...</td>\n",
       "      <td>C</td>\n",
       "      <td>a/drivers/usb/chipidea/host.c</td>\n",
       "      <td>None</td>\n",
       "      <td>b/drivers/usb/chipidea/host.c</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/torvalds/linux/commit/92947...</td>\n",
       "      <td>[PATCH] usb: chipidea: Allow disabling streami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>net/key/af_key.c</td>\n",
       "      <td>diff --git a/net/key/af_key.c b/net/key/af_key...</td>\n",
       "      <td>C</td>\n",
       "      <td>a/net/key/af_key.c</td>\n",
       "      <td>None</td>\n",
       "      <td>b/net/key/af_key.c</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/torvalds/linux/commit/a5cc6...</td>\n",
       "      <td>[PATCH] af_key: fix info leaks in notify messages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arch/x86/kernel/cpu/perf_event_intel.c</td>\n",
       "      <td>diff --git a/arch/x86/kernel/cpu/perf_event_in...</td>\n",
       "      <td>C</td>\n",
       "      <td>a/arch/x86/kernel/cpu/perf_event_intel.c</td>\n",
       "      <td>None</td>\n",
       "      <td>b/arch/x86/kernel/cpu/perf_event_intel.c</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/torvalds/linux/commit/f1923...</td>\n",
       "      <td>[PATCH] perf/x86: Fix offcore_rsp valid mask f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>tensorflow/lite/kernels/internal/reference/con...</td>\n",
       "      <td>diff --git a/tensorflow/lite/kernels/internal/...</td>\n",
       "      <td>C</td>\n",
       "      <td>a/tensorflow/lite/kernels/internal/reference/c...</td>\n",
       "      <td>None</td>\n",
       "      <td>b/tensorflow/lite/kernels/internal/reference/c...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/tensorflow/tensorflow/commi...</td>\n",
       "      <td>[PATCH] Fix a potential buffer overflow issue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>src/lib/filters.c</td>\n",
       "      <td>diff --git a/src/lib/filters.c b/src/lib/filte...</td>\n",
       "      <td>C</td>\n",
       "      <td>a/src/lib/filters.c</td>\n",
       "      <td>None</td>\n",
       "      <td>b/src/lib/filters.c</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/FreeRADIUS/freeradius-serve...</td>\n",
       "      <td>[PATCH] manual port of commit 5906bfa1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>modules/engage-paella-player/src/main/paella-o...</td>\n",
       "      <td>diff --git a/modules/engage-paella-player/src/...</td>\n",
       "      <td>HTML</td>\n",
       "      <td>a/modules/engage-paella-player/src/main/paella...</td>\n",
       "      <td>None</td>\n",
       "      <td>b/modules/engage-paella-player/src/main/paella...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/opencast/opencast/commit/d2...</td>\n",
       "      <td>[PATCH] only redirect exact hostname matches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>channels/drive/client/drive_main.c</td>\n",
       "      <td>diff --git a/channels/drive/client/drive_main....</td>\n",
       "      <td>C</td>\n",
       "      <td>a/channels/drive/client/drive_main.c</td>\n",
       "      <td>None</td>\n",
       "      <td>b/channels/drive/client/drive_main.c</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/FreeRDP/FreeRDP/commit/6655...</td>\n",
       "      <td>[PATCH] Fixed missing stream length check in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>zerver/middleware.py</td>\n",
       "      <td>diff --git a/zerver/middleware.py b/zerver/mid...</td>\n",
       "      <td>Python</td>\n",
       "      <td>a/zerver/middleware.py</td>\n",
       "      <td>None</td>\n",
       "      <td>b/zerver/middleware.py</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/zulip/zulip/commit/59edbfa4...</td>\n",
       "      <td>[PATCH] scim: Check SCIM tokens using constant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>722 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  file   \n",
       "0                                   src/libraw_cxx.cpp  \\\n",
       "1    hawtjni-runtime/src/main/java/org/fusesource/h...   \n",
       "2                          drivers/usb/chipidea/host.c   \n",
       "3                                     net/key/af_key.c   \n",
       "4               arch/x86/kernel/cpu/perf_event_intel.c   \n",
       "..                                                 ...   \n",
       "717  tensorflow/lite/kernels/internal/reference/con...   \n",
       "718                                  src/lib/filters.c   \n",
       "719  modules/engage-paella-player/src/main/paella-o...   \n",
       "720                 channels/drive/client/drive_main.c   \n",
       "721                               zerver/middleware.py   \n",
       "\n",
       "                                            patch_info programming_language   \n",
       "0    diff --git a/src/libraw_cxx.cpp b/src/libraw_c...                  C++  \\\n",
       "1    diff --git a/hawtjni-runtime/src/main/java/org...                 Java   \n",
       "2    diff --git a/drivers/usb/chipidea/host.c b/dri...                    C   \n",
       "3    diff --git a/net/key/af_key.c b/net/key/af_key...                    C   \n",
       "4    diff --git a/arch/x86/kernel/cpu/perf_event_in...                    C   \n",
       "..                                                 ...                  ...   \n",
       "717  diff --git a/tensorflow/lite/kernels/internal/...                    C   \n",
       "718  diff --git a/src/lib/filters.c b/src/lib/filte...                    C   \n",
       "719  diff --git a/modules/engage-paella-player/src/...                 HTML   \n",
       "720  diff --git a/channels/drive/client/drive_main....                    C   \n",
       "721  diff --git a/zerver/middleware.py b/zerver/mid...               Python   \n",
       "\n",
       "                                           source_file source_timestamp   \n",
       "0                                 a/src/libraw_cxx.cpp             None  \\\n",
       "1    a/hawtjni-runtime/src/main/java/org/fusesource...             None   \n",
       "2                        a/drivers/usb/chipidea/host.c             None   \n",
       "3                                   a/net/key/af_key.c             None   \n",
       "4             a/arch/x86/kernel/cpu/perf_event_intel.c             None   \n",
       "..                                                 ...              ...   \n",
       "717  a/tensorflow/lite/kernels/internal/reference/c...             None   \n",
       "718                                a/src/lib/filters.c             None   \n",
       "719  a/modules/engage-paella-player/src/main/paella...             None   \n",
       "720               a/channels/drive/client/drive_main.c             None   \n",
       "721                             a/zerver/middleware.py             None   \n",
       "\n",
       "                                           target_file target_timestamp   \n",
       "0                                 b/src/libraw_cxx.cpp             None  \\\n",
       "1    b/hawtjni-runtime/src/main/java/org/fusesource...             None   \n",
       "2                        b/drivers/usb/chipidea/host.c             None   \n",
       "3                                   b/net/key/af_key.c             None   \n",
       "4             b/arch/x86/kernel/cpu/perf_event_intel.c             None   \n",
       "..                                                 ...              ...   \n",
       "717  b/tensorflow/lite/kernels/internal/reference/c...             None   \n",
       "718                                b/src/lib/filters.c             None   \n",
       "719  b/modules/engage-paella-player/src/main/paella...             None   \n",
       "720               b/channels/drive/client/drive_main.c             None   \n",
       "721                             b/zerver/middleware.py             None   \n",
       "\n",
       "    is_binary_file                                                url   \n",
       "0            False  https://github.com/LibRaw/LibRaw/commit/2f912f...  \\\n",
       "1            False  https://github.com/fusesource/hawtjni/commit/9...   \n",
       "2            False  https://github.com/torvalds/linux/commit/92947...   \n",
       "3            False  https://github.com/torvalds/linux/commit/a5cc6...   \n",
       "4            False  https://github.com/torvalds/linux/commit/f1923...   \n",
       "..             ...                                                ...   \n",
       "717          False  https://github.com/tensorflow/tensorflow/commi...   \n",
       "718          False  https://github.com/FreeRADIUS/freeradius-serve...   \n",
       "719          False  https://github.com/opencast/opencast/commit/d2...   \n",
       "720          False  https://github.com/FreeRDP/FreeRDP/commit/6655...   \n",
       "721          False  https://github.com/zulip/zulip/commit/59edbfa4...   \n",
       "\n",
       "                                               message  \n",
       "0    [PATCH] fixed wrong data_maximum calcluation; ...  \n",
       "1              [PATCH] Simplify shared lib extraction.  \n",
       "2    [PATCH] usb: chipidea: Allow disabling streami...  \n",
       "3    [PATCH] af_key: fix info leaks in notify messages  \n",
       "4    [PATCH] perf/x86: Fix offcore_rsp valid mask f...  \n",
       "..                                                 ...  \n",
       "717  [PATCH] Fix a potential buffer overflow issue ...  \n",
       "718             [PATCH] manual port of commit 5906bfa1  \n",
       "719       [PATCH] only redirect exact hostname matches  \n",
       "720       [PATCH] Fixed missing stream length check in  \n",
       "721  [PATCH] scim: Check SCIM tokens using constant...  \n",
       "\n",
       "[722 rows x 10 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patch_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load the datasets, Large Language Model (LLM), tokenizer, and configurator. Do not worry if you do not understand yet all of those components - they will be described and discussed later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guru/miniconda/envs/fixme/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Summarize Dialogue without Prompt Engineering\n",
    "\n",
    "In this use case, you will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index). \n",
    "\n",
    "Let's upload some simple dialogues from the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.65k/4.65k [00:00<00:00, 3.59MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.3M/11.3M [00:01<00:00, 8.47MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 442k/442k [00:00<00:00, 1.03MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.35M/1.35M [00:00<00:00, 3.78MB/s]\n",
      "Generating train split: 12460 examples [00:00, 69058.37 examples/s]\n",
      "Generating validation split: 500 examples [00:00, 46219.24 examples/s]\n",
      "Generating test split: 1500 examples [00:00, 62850.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 25818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Split the DataFrame into train, validation, and test sets\n",
    "train_df = df.iloc[:800]\n",
    "validation_df = df.iloc[800:900]\n",
    "test_df = df.iloc[900:]\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "def df_to_dicts(df):\n",
    "    return [\n",
    "        {\n",
    "            'id': i,\n",
    "            'dialogue': row['code_before'],\n",
    "            'summary': row['code_after'],\n",
    "            'topic': '', # Optional field\n",
    "        }\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_dict({'id': list(train_df.index), 'dialogue': train_df['code_before'], 'summary': train_df['code_after'], 'topic': [''] * len(train_df)})\n",
    "validation_dataset = Dataset.from_dict({'id': list(validation_df.index), 'dialogue': validation_df['code_before'], 'summary': validation_df['code_after'], 'topic': [''] * len(validation_df)})\n",
    "test_dataset = Dataset.from_dict({'id': list(test_df.index), 'dialogue': test_df['code_before'], 'summary': test_df['code_after'], 'topic': [''] * len(test_df)})\n",
    "\n",
    "# Create DatasetDict with the desired format\n",
    "mydataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Example  1\n",
      "-------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "\tif (!skb)\n",
      "\t\treturn err;\n",
      "\n",
      "\tmsg->msg_namelen = 0;\n",
      "\n",
      "\tcopied = skb->len;\n",
      "\tif (len < copied) {\n",
      "\t\tmsg->msg_flags |= MSG_TRUNC;\n",
      "\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\tif (!skb)\n",
      "\t\treturn err;\n",
      "\n",
      "\tcopied = skb->len;\n",
      "\tif (len < copied) {\n",
      "\t\tmsg->msg_flags |= MSG_TRUNC;\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "Example  2\n",
      "-------------------------------------------------\n",
      "INPUT DIALOGUE:\n",
      "\t\tsin->sin_port = udp_hdr(skb)->source;\n",
      "\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n",
      "\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n",
      "\t}\n",
      "\tif (inet->cmsg_flags)\n",
      "\t\tip_cmsg_recv(msg, skb);\n",
      "\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\t\tsin->sin_port = udp_hdr(skb)->source;\n",
      "\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n",
      "\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n",
      "\t\t*addr_len = sizeof(*sin);\n",
      "\t}\n",
      "\tif (inet->cmsg_flags)\n",
      "\t\tip_cmsg_recv(msg, skb);\n",
      "\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [40, 200]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(50))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT DIALOGUE:')\n",
    "    print(mydataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY:')\n",
    "    print(mydataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Print a couple of dialogues with their baseline summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAYlS40Z3l-v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name='google/flan-t5-base'\n",
    "model_name='Salesforce/codet5-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPqQA3TT3l_I",
    "tags": []
   },
   "source": [
    "To perform encoding and decoding, you need to work with text in a tokenized form. **Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models. \n",
    "\n",
    "Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Test the tokenizer encoding and decoding a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5p-220m\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5p-220m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": %s: %\n"
     ]
    }
   ],
   "source": [
    "text = \"def greet(user): print(f'hello <extra_id_0>!')\"\n",
    "# text = \"def add(a, b): \\n int sum= a + b \\n return sum\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# simply generate a single sequence\n",
    "generated_ids = model.generate(input_ids, max_length=8)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "# this prints \"{user.username}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering. **Prompt engineering** is an act of a human changing the **prompt** (input) to improve the response for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Example  1\n",
      "-------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\tif (!skb)\n",
      "\t\treturn err;\n",
      "\n",
      "\tmsg->msg_namelen = 0;\n",
      "\n",
      "\tcopied = skb->len;\n",
      "\tif (len < copied) {\n",
      "\t\tmsg->msg_flags |= MSG_TRUNC;\n",
      "\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\tif (!skb)\n",
      "\t\treturn err;\n",
      "\n",
      "\tcopied = skb->len;\n",
      "\tif (len < copied) {\n",
      "\t\tmsg->msg_flags |= MSG_TRUNC;\n",
      "\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "/*\n",
      " * Copyright (c) 2008-2021, Hazelcast, Inc. All Rights Reserved.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not\n",
      "\n",
      "-------------------------------------------------\n",
      "Example  2\n",
      "-------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\t\tsin->sin_port = udp_hdr(skb)->source;\n",
      "\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n",
      "\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n",
      "\t}\n",
      "\tif (inet->cmsg_flags)\n",
      "\t\tip_cmsg_recv(msg, skb);\n",
      "\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\t\tsin->sin_port = udp_hdr(skb)->source;\n",
      "\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n",
      "\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n",
      "\t\t*addr_len = sizeof(*sin);\n",
      "\t}\n",
      "\tif (inet->cmsg_flags)\n",
      "\t\tip_cmsg_recv(msg, skb);\n",
      "\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\n",
      "/*\n",
      " * Copyright (c) 2008-2021, Hazelcast, Inc. All Rights Reserved.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = mydataset['test'][index]['dialogue']\n",
    "    summary = mydataset['test'][index]['summary']\n",
    "    \n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the guesses of the model make some sense, but it doesn't seem to be sure what task it is supposed to accomplish. Seems it just makes up the next sentence in the dialogue. Prompt engineering can help here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Summarize Dialogue with an Instruction Prompt\n",
    "\n",
    "Prompt engineering is an important concept in using foundation models for text generation. You can check out [this blog](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering) from Amazon Science for a quick introduction to prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 - Zero Shot Inference with an Instruction Prompt\n",
    "\n",
    "In order to instruct the model to perform a task - summarize a dialogue - you can take the dialogue and convert it into an instruction prompt. This is often called **zero shot inference**.  You can check out [this blog from AWS](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/) for a quick description of what zero shot learning is and why it is an important concept to the LLM model.\n",
    "\n",
    "Wrap the dialogue in a descriptive instruction and see how the generated text will change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Example  1\n",
      "-------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Vulnerable C program:\n",
      "\n",
      "\tif (!skb)\n",
      "\t\treturn err;\n",
      "\n",
      "\tmsg->msg_namelen = 0;\n",
      "\n",
      "\tcopied = skb->len;\n",
      "\tif (len < copied) {\n",
      "\t\tmsg->msg_flags |= MSG_TRUNC;\n",
      "\n",
      "\n",
      "Patch of the program:\n",
      "    \n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\tif (!skb)\n",
      "\t\treturn err;\n",
      "\n",
      "\tcopied = skb->len;\n",
      "\tif (len < copied) {\n",
      "\t\tmsg->msg_flags |= MSG_TRUNC;\n",
      "\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "/*\n",
      " * Copyright (c) 2008-2021, Hazelcast, Inc. All Rights Reserved.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not use this file except in compliance with the License.\n",
      " * You may obtain a copy of the License at\n",
      " *\n",
      " * http://www.apache.org/licenses/LICENSE-2.0\n",
      " *\n",
      " * Unless required by\n",
      "\n",
      "-------------------------------------------------\n",
      "Example  2\n",
      "-------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Vulnerable C program:\n",
      "\n",
      "\t\tsin->sin_port = udp_hdr(skb)->source;\n",
      "\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n",
      "\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n",
      "\t}\n",
      "\tif (inet->cmsg_flags)\n",
      "\t\tip_cmsg_recv(msg, skb);\n",
      "\n",
      "\n",
      "Patch of the program:\n",
      "    \n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\t\tsin->sin_port = udp_hdr(skb)->source;\n",
      "\t\tsin->sin_addr.s_addr = ip_hdr(skb)->saddr;\n",
      "\t\tmemset(sin->sin_zero, 0, sizeof(sin->sin_zero));\n",
      "\t\t*addr_len = sizeof(*sin);\n",
      "\t}\n",
      "\tif (inet->cmsg_flags)\n",
      "\t\tip_cmsg_recv(msg, skb);\n",
      "\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "/*\n",
      " * Copyright (c) 2008-2021, Hazelcast, Inc. All Rights Reserved.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not use this file except in compliance with the License.\n",
      " * You may obtain a copy of the License at\n",
      " *\n",
      " * http://www.apache.org/licenses/LICENSE-2.0\n",
      " *\n",
      " * Unless required by\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = mydataset['test'][index]['dialogue']\n",
    "    summary = mydataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Vulnerable C program:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Patch of the program:\n",
    "    \"\"\"\n",
    "\n",
    "    # Input constructed prompt instead of the dialogue.\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=100,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)    \n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.15, 0.02531645569620253, 0.01282051282051282, 0.0],\n",
       " 'brevity_penalty': 0.9162188716508777,\n",
       " 'length_ratio': 0.9195402298850575,\n",
       " 'translation_length': 80,\n",
       " 'reference_length': 87}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "# summary\n",
    "# output\n",
    "bleu = evaluate.load('bleu')\n",
    "bleu.compute(predictions=[output], references=[summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pass@1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "\n",
    "code_eval = load(\"code_eval\")\n",
    "test_cases = [summary]\n",
    "candidates = [[output]]\n",
    "pass_at_k, results = code_eval.compute(references=test_cases, predictions=candidates, k=[1 ,5, 10])\n",
    "print(pass_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5}\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "super_glue_metric = load('super_glue', 'copa') \n",
    "predictions = [0, 1]\n",
    "references = [0, 0]\n",
    "results = super_glue_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better! But the model still does not pick up on the nuance of the conversations though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "- Experiment with the `prompt` text and see how the inferences will be changed. Will the inferences change if you end the prompt with just empty string vs. `Summary: `?\n",
    "- Try to rephrase the beginning of the `prompt` text from `Summarize the following conversation.` to something different - and see how it will influence the generated output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5\n",
    "\n",
    "Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/tree/main/flan/v2). In the following code, you will use one of the [pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Vulnerable C program:\n",
      "\n",
      "int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)\n",
      "{\n",
      "\tstruct au1100fb_device *fbdev;\n",
      "\tunsigned int len;\n",
      "\tunsigned long start=0, off;\n",
      "\n",
      "\tfbdev = to_au1100fb_device(fbi);\n",
      "\n",
      "\tif (vma->vm_pgoff > (~0UL >> PAGE_SHIFT)) {\n",
      "\t\treturn -EINVAL;\n",
      "\t}\n",
      "\n",
      "\tstart = fbdev->fb_phys & PAGE_MASK;\n",
      "\tlen = PAGE_ALIGN((start & ~PAGE_MASK) + fbdev->fb_len);\n",
      "\n",
      "\toff = vma->vm_pgoff << PAGE_SHIFT;\n",
      "\n",
      "\tif ((vma->vm_end - vma->vm_start + off) > len) {\n",
      "\t\treturn -EINVAL;\n",
      "\t}\n",
      "\n",
      "\toff += start;\n",
      "\tvma->vm_pgoff = off >> PAGE_SHIFT;\n",
      "\n",
      "\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n",
      "\tpgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6\n",
      "\n",
      "\tif (io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,\n",
      "\t\t\t\tvma->vm_end - vma->vm_start,\n",
      "\t\t\t\tvma->vm_page_prot)) {\n",
      "\t\treturn -EAGAIN;\n",
      "\t}\n",
      "\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "static struct fb_ops au1100fb_ops =\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)\n",
      "{\n",
      "\tstruct au1100fb_device *fbdev;\n",
      "\n",
      "\tfbdev = to_au1100fb_device(fbi);\n",
      "\n",
      "\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n",
      "\tpgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6\n",
      "\n",
      "\treturn vm_iomap_memory(vma, fbdev->fb_phys, fbdev->fb_len);\n",
      "}\n",
      "\n",
      "static struct fb_ops au1100fb_ops =\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "( void *) ;( void * )( void * )( void * )( void * )( void * )( void * )( void * )( void * )(\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Vulnerable C program:\n",
      "\n",
      "\t/* First pass: copy the tree topology */\n",
      "\tcopy_flags = CL_COPY_ALL | CL_EXPIRE;\n",
      "\tif (user_ns != mnt_ns->user_ns)\n",
      "\t\tcopy_flags |= CL_SHARED_TO_SLAVE;\n",
      "\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n",
      "\tif (IS_ERR(new)) {\n",
      "\t\tup_write(&namespace_sem);\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\t/* First pass: copy the tree topology */\n",
      "\tcopy_flags = CL_COPY_ALL | CL_EXPIRE;\n",
      "\tif (user_ns != mnt_ns->user_ns)\n",
      "\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n",
      "\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n",
      "\tif (IS_ERR(new)) {\n",
      "\t\tup_write(&namespace_sem);\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "new->mnt.mnt_root, copy_flags);new->mnt.mnt_root, copy_flags); }new->mnt.mnt_root, copy_flags);new->mnt.mnt_root,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = mydataset['test'][index]['dialogue']\n",
    "    summary = mydataset['test'][index]['summary']\n",
    "        \n",
    "    prompt = f\"\"\"\n",
    "Vulnerable C program:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What is the patch of the program?\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(\n",
    "        model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_new_tokens=50,\n",
    "        )[0], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{prompt}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. This is what you will try to solve with the few shot inferencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Summarize Dialogue with One Shot and Few Shot Inference\n",
    "\n",
    "**One shot and few shot inference** are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task.  You can read more about it in [this blog from HuggingFace](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='4.1'></a>\n",
    "### 4.1 - One Shot Inference\n",
    "\n",
    "Let's build a function that takes a list of `example_indices_full`, generates a prompt with full examples, then at the end appends the prompt which you want the model to complete (`example_index_to_summarize`).  You will use the same FLAN-T5 prompt template from section [3.2](#3.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = mydataset['test'][index]['dialogue']\n",
    "        summary = mydataset['test'][index]['summary']\n",
    "        \n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Vulerable C program:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What is the patch of the program?\n",
    "\n",
    "{summary}\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    dialogue = mydataset['test'][example_index_to_summarize]['dialogue']\n",
    "    \n",
    "    prompt += f\"\"\"\n",
    "Vulerable C program:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What is the patch of the program?\n",
    "\"\"\"\n",
    "        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Construct the prompt to perform one shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vulerable C program:\n",
      "\n",
      "int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)\n",
      "{\n",
      "\tstruct au1100fb_device *fbdev;\n",
      "\tunsigned int len;\n",
      "\tunsigned long start=0, off;\n",
      "\n",
      "\tfbdev = to_au1100fb_device(fbi);\n",
      "\n",
      "\tif (vma->vm_pgoff > (~0UL >> PAGE_SHIFT)) {\n",
      "\t\treturn -EINVAL;\n",
      "\t}\n",
      "\n",
      "\tstart = fbdev->fb_phys & PAGE_MASK;\n",
      "\tlen = PAGE_ALIGN((start & ~PAGE_MASK) + fbdev->fb_len);\n",
      "\n",
      "\toff = vma->vm_pgoff << PAGE_SHIFT;\n",
      "\n",
      "\tif ((vma->vm_end - vma->vm_start + off) > len) {\n",
      "\t\treturn -EINVAL;\n",
      "\t}\n",
      "\n",
      "\toff += start;\n",
      "\tvma->vm_pgoff = off >> PAGE_SHIFT;\n",
      "\n",
      "\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n",
      "\tpgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6\n",
      "\n",
      "\tif (io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,\n",
      "\t\t\t\tvma->vm_end - vma->vm_start,\n",
      "\t\t\t\tvma->vm_page_prot)) {\n",
      "\t\treturn -EAGAIN;\n",
      "\t}\n",
      "\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "static struct fb_ops au1100fb_ops =\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n",
      "int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)\n",
      "{\n",
      "\tstruct au1100fb_device *fbdev;\n",
      "\n",
      "\tfbdev = to_au1100fb_device(fbi);\n",
      "\n",
      "\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n",
      "\tpgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6\n",
      "\n",
      "\treturn vm_iomap_memory(vma, fbdev->fb_phys, fbdev->fb_len);\n",
      "}\n",
      "\n",
      "static struct fb_ops au1100fb_ops =\n",
      "\n",
      "\n",
      "\n",
      "Vulerable C program:\n",
      "\n",
      "\t/* First pass: copy the tree topology */\n",
      "\tcopy_flags = CL_COPY_ALL | CL_EXPIRE;\n",
      "\tif (user_ns != mnt_ns->user_ns)\n",
      "\t\tcopy_flags |= CL_SHARED_TO_SLAVE;\n",
      "\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n",
      "\tif (IS_ERR(new)) {\n",
      "\t\tup_write(&namespace_sem);\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now pass this prompt to perform the one shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\t/* First pass: copy the tree topology */\n",
      "\tcopy_flags = CL_COPY_ALL | CL_EXPIRE;\n",
      "\tif (user_ns != mnt_ns->user_ns)\n",
      "\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n",
      "\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n",
      "\tif (IS_ERR(new)) {\n",
      "\t\tup_write(&namespace_sem);\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ONE SHOT:\n",
      "(vma)(vma)->->-> vm_flags |=-> vm_flags |=-> vm_flags-> vm_flags-> vm_flags |=-> vm_flags |=-> vm_flags |=-> vm_flags-> vm_flags |=-> vm_flags-> vm_flags |=-> vm_flags |=-> vm_flags-> vm_flags |=-> vm_flags\n"
     ]
    }
   ],
   "source": [
    "summary = mydataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=100,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 - Few Shot Inference\n",
    "\n",
    "Let's explore few shot inference by adding two more full dialogue-summary pairs to your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vulerable C program:\n",
      "\n",
      "\trcu_assign_pointer(vq->private_data, oldsock);\n",
      "\tvhost_net_enable_vq(n, vq);\n",
      "\tif (ubufs)\n",
      "\t\tvhost_net_ubuf_put_and_wait(ubufs);\n",
      "err_ubufs:\n",
      "\tfput(sock->file);\n",
      "err_vq:\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n",
      "\trcu_assign_pointer(vq->private_data, oldsock);\n",
      "\tvhost_net_enable_vq(n, vq);\n",
      "\tif (ubufs)\n",
      "\t\tvhost_net_ubuf_put_wait_and_free(ubufs);\n",
      "err_ubufs:\n",
      "\tfput(sock->file);\n",
      "err_vq:\n",
      "\n",
      "\n",
      "\n",
      "Vulerable C program:\n",
      "\n",
      "\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n",
      "\tstruct pmu *leader_pmu = event->group_leader->pmu;\n",
      "\n",
      "\tif (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)\n",
      "\t\treturn 1;\n",
      "\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n",
      "\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n",
      "\tstruct pmu *leader_pmu = event->group_leader->pmu;\n",
      "\n",
      "\tif (is_software_event(event))\n",
      "\t\treturn 1;\n",
      "\n",
      "\tif (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)\n",
      "\t\treturn 1;\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Vulerable C program:\n",
      "\n",
      "    pkiDebug(\"found kdcPkId in AS REQ\\n\");\n",
      "    is = d2i_PKCS7_ISSUER_AND_SERIAL(NULL, &p, (int)pkid_len);\n",
      "    if (is == NULL)\n",
      "        goto cleanup;\n",
      "\n",
      "    status = X509_NAME_cmp(X509_get_issuer_name(kdc_cert), is->issuer);\n",
      "    if (!status) {\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n",
      "    pkiDebug(\"found kdcPkId in AS REQ\\n\");\n",
      "    is = d2i_PKCS7_ISSUER_AND_SERIAL(NULL, &p, (int)pkid_len);\n",
      "    if (is == NULL)\n",
      "        return retval;\n",
      "\n",
      "    status = X509_NAME_cmp(X509_get_issuer_name(kdc_cert), is->issuer);\n",
      "    if (!status) {\n",
      "\n",
      "\n",
      "\n",
      "Vulerable C program:\n",
      "\n",
      "    }\n",
      "    return -1;\n",
      "  }\n",
      "  int64_t pid = -1;\n",
      "  sscanf(buf, \"%\" PRId64, &pid);\n",
      "  assert(pid);\n",
      "  return (pid_t)pid;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "What is the patch of the program?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [30, 10, 150]\n",
    "example_index_to_summarize = 300\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now pass this prompt to perform a few shot inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Prompt:\n",
      "    }\n",
      "    return -1;\n",
      "  }\n",
      "  int64_t pid = -1;\n",
      "  sscanf(buf, \"%\" PRId64, &pid);\n",
      "  assert(pid);\n",
      "  return (pid_t)pid;\n",
      "}\n",
      "\n",
      "\n",
      "-------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "    }\n",
      "    return -1;\n",
      "  }\n",
      "  always_assert(buf == \"success\");\n",
      "  int64_t pid = -1;\n",
      "  lwp_read_int64(fin, pid);\n",
      "  always_assert(pid);\n",
      "  return (pid_t)pid;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      " return -1;  int64_t, pid = -1; sscanf(buf, \"%\" PRId64, &pid); assert(pid); return (pid_t)pid; \n"
     ]
    }
   ],
   "source": [
    "\n",
    "dialogue = mydataset['test']['dialogue'][example_index_to_summarize]\n",
    "summary = mydataset['test']['summary'][example_index_to_summarize]\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=100,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(dash_line)\n",
    "print(f'Prompt:\\n{dialogue}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this case, few shot did not provide much of an improvement over one shot inference.  And, anything above 5 or 6 shot will typically not help much, either.  Also, you need to make sure that you do not exceed the model's input-context length which, in our case, if 512 tokens.  Anything above the context length will be ignored.\n",
    "\n",
    "However, you can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "Experiment with the few shot inferencing.\n",
    "- Choose different dialogues - change the indices in the `example_indices_full` list and `example_index_to_summarize` value.\n",
    "- Change the number of shots. Be sure to stay within the model's 512 context length, however.\n",
    "\n",
    "How well does few shot inferencing work with other examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Generative Configuration Parameters for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig). \n",
    "\n",
    "A convenient way of organizing the configuration parameters is to use `GenerationConfig` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Exercise:**\n",
    "\n",
    "Change the configuration parameters to investigate their influence on the output. \n",
    "\n",
    "Putting the parameter `do_sample = True`, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`). \n",
    "\n",
    "Uncomment the lines in the cell below and rerun the code. Try to analyze the results. You can read some comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FEW SHOT:\n",
      "According to personal views by Person 2, Trump deserves re presidency, especially Trump, but does think Trump needs to be more of an outtake from the country. He will do not win by giving the US only the President of the U.S.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is crazy for Trump and voted for him. #Person2# doesn't agree with #Person1# on Trump and will vote for Biden.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generation_config = GenerationConfig(max_new_tokens=50)\n",
    "# generation_config = GenerationConfig(max_new_tokens=10)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
    "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
    "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=2.0)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        generation_config=generation_config,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments related to the choice of the parameters in the code cell above:\n",
    "- Choosing `max_new_tokens=10` will make the output text too short, so the dialogue summary will be cut.\n",
    "- Putting `do_sample = True` and changing the temperature value you get more flexibility in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, prompt engineering can take you a long way for this use case, but there are some limitations. Next, you will start to explore how you can use fine-tuning to help your LLM to understand a particular use case in better depth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Fine-Tune a Generative AI Model for Dialogue Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 222882048\n",
      "all model parameters: 222882048\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "\t/* First pass: copy the tree topology */\n",
      "\tcopy_flags = CL_COPY_ALL | CL_EXPIRE;\n",
      "\tif (user_ns != mnt_ns->user_ns)\n",
      "\t\tcopy_flags |= CL_SHARED_TO_SLAVE;\n",
      "\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n",
      "\tif (IS_ERR(new)) {\n",
      "\t\tup_write(&namespace_sem);\n",
      "\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\t/* First pass: copy the tree topology */\n",
      "\tcopy_flags = CL_COPY_ALL | CL_EXPIRE;\n",
      "\tif (user_ns != mnt_ns->user_ns)\n",
      "\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n",
      "\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n",
      "\tif (IS_ERR(new)) {\n",
      "\t\tup_write(&namespace_sem);\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "new; }new; }new; }new; }new; }\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = mydataset['test'][index]['dialogue']\n",
    "summary = mydataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [00:00<00:00, 1773.04 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1868.36 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2172/2172 [00:00<00:00, 2198.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = mydataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 76.73 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 180.73 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.17 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (1, 2)\n",
      "Validation: (1, 2)\n",
      "Test: (1, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Fine-Tune the Model with the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'models/vul-fix-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guru/miniconda/envs/fixme/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:11<00:00, 491.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.625, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 491.2789, 'train_samples_per_second': 0.016, 'train_steps_per_second': 0.002, 'train_loss': 3.625, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=3.625, metrics={'train_runtime': 491.2789, 'train_samples_per_second': 0.016, 'train_steps_per_second': 0.002, 'train_loss': 3.625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/instruct_model-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model(f'models/instruct_model-{output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForSeq2SeqLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m instruct_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/instruct_model-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForSeq2SeqLM' is not defined"
     ]
    }
   ],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(f'models/instruct_model-{output_dir}', torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "\t/* First pass: copy the tree topology */\n",
      "\tcopy_flags = CL_COPY_ALL | CL_EXPIRE;\n",
      "\tif (user_ns != mnt_ns->user_ns)\n",
      "\t\tcopy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;\n",
      "\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n",
      "\tif (IS_ERR(new)) {\n",
      "\t\tup_write(&namespace_sem);\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "if (IS_ERR(old)){\n",
      "\n",
      "Summary:; } }new->mnt.mnt_root, new->mnt.mnt_root, copy_flags);new->mnt.mnt_root, copy_flags); }new->mnt.mnt_root, copy_flags); }(new) { if(IS_ERR(new)) {new->mnt.mnt_root; }(new) { if\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INSTRUCT MODEL:\n",
      "new; }new; }new; }new; }new; }\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = mydataset['test'][index]['dialogue']\n",
    "human_baseline_summary = mydataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
    "print(dash_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>instruct_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tunsigned long mq_bytes;\\t/* How many bytes c...</td>\n",
       "      <td>*/*/****** ** ** * ** ** * ** ** * ** ** * ** ...</td>\n",
       "      <td>*/*/))))) ;) ;) ;) ;) ;) ;) ;) ; }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tsock_wfree(skb);\\n}\\n\\n/*\\n * The \"user-&gt;uni...</td>\n",
       "      <td>(structstruct sk_buff *skb)skb)</td>\n",
       "      <td>skbskb)skb) {skb)skb) {skb)skb) { skb_queue_ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tunsigned char max_level = 0;\\n\\tint unix_soc...</td>\n",
       "      <td>sock_sk(sk);sock_count =sock_count;sock_count;...</td>\n",
       "      <td>sock_get_name(sk);sock_get_name(sk);sock_get_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tif (!UNIXCB(skb).fp)\\n\\t\\treturn -ENOMEM;\\n\\...</td>\n",
       "      <td>(scm-&gt;fp-&gt;count) {if (scm-&gt;fp-&gt;count)\\nForeach...</td>\n",
       "      <td>{=(skb).fp;if(!UNIXCB(skb).fp)\\n\\nif (unix_soc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\\n\\tstruct sock *s = unix_get_socket(fp);\\n\\n...</td>\n",
       "      <td>{\\n\\nIntegrity:{}} }}} }{(s) {(s) {(s)</td>\n",
       "      <td>{\\n\\nif (atomic_long_inc_return(&amp;u-&gt;inflight) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\t\\t\\tBUG_ON(list_empty(&amp;u-&gt;link));\\n\\t\\t}\\n\\t...</td>\n",
       "      <td>if (atomic_long_dec_and_test(&amp;u-&gt;inflight))\\n\\...</td>\n",
       "      <td>(x) {(x) {(x) {(x) {(x) {(x) {(x) {(x) {(x) {(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\tltv_t                   *pLtv;\\n\\tbool_t    ...</td>\n",
       "      <td>HCF_PORT_0  functionHCF_PORT_0HCF_PORT_0HCF_PO...</td>\n",
       "      <td>HCF_PORT_0;HCF_PORT_0;HCF_PORT_0;HCF_PORT_0;HC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tcase CFG_CNF_OWN_NAM...</td>\n",
       "      <td>(void *)pLtv-&gt;u.u16[0];(void *)pLtv-&gt;u.u16[0];...</td>\n",
       "      <td>CNV_INT_TO_LITTLE(pLtv-&gt;u.u16[0]);CNV_INT_TO_L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{\\n\\tstruct wl_private *lp = wl_priv(dev);\\n\\t...</td>\n",
       "      <td>{flags = wl_get_cond_volatile(dev);devflags=lp...</td>\n",
       "      <td>flagsflags = 0;flags =flagsflagsflagsflags =fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\twl_lock(lp, &amp;flags);\\n\\n\\tmemset(lp-&gt;Station...</td>\n",
       "      <td>(wrqu-&gt;data)get\\n\\nif (wrqu-&gt;data.length) {(lp...</td>\n",
       "      <td>((( lp, &amp;flags);</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries   \n",
       "0  \\tunsigned long mq_bytes;\\t/* How many bytes c...  \\\n",
       "1  \\tsock_wfree(skb);\\n}\\n\\n/*\\n * The \"user->uni...   \n",
       "2  \\tunsigned char max_level = 0;\\n\\tint unix_soc...   \n",
       "3  \\tif (!UNIXCB(skb).fp)\\n\\t\\treturn -ENOMEM;\\n\\...   \n",
       "4  {\\n\\tstruct sock *s = unix_get_socket(fp);\\n\\n...   \n",
       "5  \\t\\t\\tBUG_ON(list_empty(&u->link));\\n\\t\\t}\\n\\t...   \n",
       "6  \\tltv_t                   *pLtv;\\n\\tbool_t    ...   \n",
       "7  \\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tcase CFG_CNF_OWN_NAM...   \n",
       "8  {\\n\\tstruct wl_private *lp = wl_priv(dev);\\n\\t...   \n",
       "9  \\twl_lock(lp, &flags);\\n\\n\\tmemset(lp->Station...   \n",
       "\n",
       "                            original_model_summaries   \n",
       "0  */*/****** ** ** * ** ** * ** ** * ** ** * ** ...  \\\n",
       "1                    (structstruct sk_buff *skb)skb)   \n",
       "2  sock_sk(sk);sock_count =sock_count;sock_count;...   \n",
       "3  (scm->fp->count) {if (scm->fp->count)\\nForeach...   \n",
       "4             {\\n\\nIntegrity:{}} }}} }{(s) {(s) {(s)   \n",
       "5  if (atomic_long_dec_and_test(&u->inflight))\\n\\...   \n",
       "6  HCF_PORT_0  functionHCF_PORT_0HCF_PORT_0HCF_PO...   \n",
       "7  (void *)pLtv->u.u16[0];(void *)pLtv->u.u16[0];...   \n",
       "8  {flags = wl_get_cond_volatile(dev);devflags=lp...   \n",
       "9  (wrqu->data)get\\n\\nif (wrqu->data.length) {(lp...   \n",
       "\n",
       "                            instruct_model_summaries  \n",
       "0                 */*/))))) ;) ;) ;) ;) ;) ;) ;) ; }  \n",
       "1  skbskb)skb) {skb)skb) {skb)skb) { skb_queue_ta...  \n",
       "2  sock_get_name(sk);sock_get_name(sk);sock_get_n...  \n",
       "3  {=(skb).fp;if(!UNIXCB(skb).fp)\\n\\nif (unix_soc...  \n",
       "4  {\\n\\nif (atomic_long_inc_return(&u->inflight) ...  \n",
       "5  (x) {(x) {(x) {(x) {(x) {(x) {(x) {(x) {(x) {(...  \n",
       "6  HCF_PORT_0;HCF_PORT_0;HCF_PORT_0;HCF_PORT_0;HC...  \n",
       "7  CNV_INT_TO_LITTLE(pLtv->u.u16[0]);CNV_INT_TO_L...  \n",
       "8  flagsflags = 0;flags =flagsflagsflagsflags =fl...  \n",
       "9                                   ((( lp, &flags);  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues = mydataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = mydataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_model_text_output)\n",
    "    \n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n",
    "\n",
    "human_baseline_summaries = results['human_baseline_summaries'].values\n",
    "original_model_summaries = results['original_model_summaries'].values\n",
    "instruct_model_summaries = results['instruct_model_summaries'].values\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('INSTRUCT MODEL:')\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(instruct_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "fixme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
