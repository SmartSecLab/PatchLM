# ### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)
import numpy as np
import pandas as pd
import evaluate
from transformers import GenerationConfig

# custom imports
from source.prompt import zero_prompt
import source.utility as util

# Setup logger
log = util.get_logger()
config = util.load_config()

rouge = evaluate.load("rouge")
dash_line = "=" * 50


def get_trainable_model_pars(model):
    trainable_model_params = 0
    all_model_params = 0
    for _, param in model.named_parameters():
        all_model_params += param.numel()
        if param.requires_grad:
            trainable_model_params += param.numel()
    percentage = 100 * trainable_model_params / all_model_params

    return (
        f"Trainable model parameters: {trainable_model_params}\n"
        f"All model parameters: {all_model_params}\n"
        f"Percentage of trainable model parameters: {percentage:.2f}%"
    )


def show_original_instruct_fix(
    dataset, tokenizer, original_model, instruct_model, index=2
):
    prompt = zero_prompt(dataset, index=index)
    human_baseline_fix = dataset["test"][index]["fix"]

    input_ids = tokenizer(prompt, return_tensors="pt").input_ids

    original_model_outputs = original_model.generate(
        input_ids=input_ids,
        generation_config=GenerationConfig(
            max_new_tokens=config["generation"]["max_new_tokens"],
            num_beams=config["generation"]["num_beams"],
        ),
    )
    original_model_text_output = tokenizer.decode(
        original_model_outputs[0], skip_special_tokens=True
    )

    instruct_model_outputs = instruct_model.generate(
        input_ids=input_ids,
        generation_config=GenerationConfig(
            max_new_tokens=config["generation"]["max_new_tokens"],
            num_beams=config["generation"]["num_beams"],
        ),
    )
    instruct_model_text_output = tokenizer.decode(
        instruct_model_outputs[0], skip_special_tokens=True
    )

    log.info(dash_line)
    log.info(f"BASELINE PATCH:\n{human_baseline_fix}")
    log.info(dash_line)
    log.info(f"ORIGINAL MODEL:\n{original_model_text_output}")
    log.info(dash_line)
    log.info(f"INSTRUCT MODEL:\n{instruct_model_text_output}")


def evaluate_rouge(results):
    """ Evaluate the fixes generated by the models using the ROUGE metric """
    human_baseline_fixes = results["human_baseline_fixes"].values
    original_model_fixes = results["original_model_fixes"].values
    instruct_model_fixes = results["instruct_model_fixes"].values

    original_model_results = rouge.compute(
        predictions=original_model_fixes,
        references=human_baseline_fixes[0: len(original_model_fixes)],
        use_aggregator=True,
        use_stemmer=True,
    )

    instruct_model_results = rouge.compute(
        predictions=instruct_model_fixes,
        references=human_baseline_fixes[0: len(instruct_model_fixes)],
        use_aggregator=True,
        use_stemmer=True,
    )

    log.info("ORIGINAL MODEL:")
    log.info(original_model_results)
    log.info("INSTRUCT MODEL:")
    log.info(instruct_model_results)

    log.info("Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL")

    improvement = np.array(list(instruct_model_results.values())) - np.array(
        list(original_model_results.values())
    )
    for key, value in zip(instruct_model_results.keys(), improvement):
        log.info(f"{key}: {value*100:.2f}%")


def generate_fixes(
    original_model,
    instruct_model,
    tokenizer,
    vulnerables,
    human_baseline_fixes,
    result_csv,
):
    """" Generate fixes for a list of vulnerables using a model """
    original_model_fixes = []
    instruct_model_fixes = []
    for _, vulnerable in enumerate(vulnerables):
        prompt = f"""
                    Generation the fix for the following vulnerable code:

                    {vulnerable}

                    fix: \n"""
        input_ids = tokenizer(prompt, return_tensors="pt").input_ids

        original_model_outputs = original_model.generate(
            input_ids=input_ids, generation_config=GenerationConfig(
                max_new_tokens=config["generation"]["max_new_tokens"],)
        )
        original_model_text_output = tokenizer.decode(
            original_model_outputs[0], skip_special_tokens=True
        )

        original_model_fixes.append(original_model_text_output)

        instruct_model_outputs = instruct_model.generate(
            input_ids=input_ids, generation_config=GenerationConfig(
                max_new_tokens=config["generation"]["max_new_tokens"],
            )
        )
        instruct_model_text_output = tokenizer.decode(
            instruct_model_outputs[0], skip_special_tokens=True
        )

        instruct_model_fixes.append(instruct_model_text_output)

    zipped_fixes = list(
        zip(
            human_baseline_fixes, original_model_fixes, instruct_model_fixes
        )
    )

    df = pd.DataFrame(
        zipped_fixes,
        columns=[
            "human_baseline_fixes",
            "original_model_fixes",
            "instruct_model_fixes",
        ],
    )
    df.to_csv(result_csv, index=False)
    log.info(dash_line)
    log.info(f"Results of vul-fix-training saved to {result_csv}")
    log.info(dash_line)
    log.info("Sample of the results:")
    log.info(df.head())
    log.info(dash_line)
    return df
