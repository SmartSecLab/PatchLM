{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering using CodeLlamma API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Assuming you're using CUDA, set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f'Available device: {device}')\n",
    "\n",
    "\n",
    "# most lightweight model of CodeLlama for instruction prompt\n",
    "# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "# base_model = 'QuantFactory/CodeLlama-7b-hf-GGUF'\n",
    "# model_id = '/Users/guru/research/LLMs/CodeLlama-70-Instruct-hf'\n",
    "base_model = 'models/CodeLLama-7b-quantized-4bit'\n",
    "output_dir = 'models/patch-code-llama/checkpoint-400'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose between 4-bit or 8-bit quantization\n",
    "use_4bit = True  # Set to False for 8-bit quantization\n",
    "\n",
    "# Configure quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,  \n",
    "    # load_in_8bit_fp32_cpu_offload=not use_4bit\n",
    ")\n",
    "\n",
    "# Initialize the model with empty weights\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        quantization_config=quantization_config,\n",
    "        device_map='auto',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_4bit:\n",
    "    model.save_pretrained('models/Model-7b-quantized-4bit')\n",
    "    tokenizer.save_pretrained('models/Model-7b-quantized-4bit')\n",
    "else:\n",
    "    model.save_pretrained('models/Model-7b-quantized-8bit')\n",
    "    tokenizer.save_pretrained('models/Model-7b-quantized-8bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_model= AutoModelForCausalLM.from_pretrained(\n",
    "#     'models/CodeLLama-Debug', \n",
    "#     quantization_config=quantization_config, \n",
    "#     device_map='auto'\n",
    "#     )\n",
    "# model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "instruct_model = 'models/CodeLLama-Debug'\n",
    "\n",
    "\n",
    "\n",
    "instruct_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=instruct_model,\n",
    "    device_map='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/miniconda3/envs/patch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['vulnerable', 'fix', 'question'],\n",
      "        num_rows: 64643\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['vulnerable', 'fix', 'question'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load ir1xor1\n",
    "dataset = load_dataset(\"ASSERT-KTH/repairllama-datasets\", \"ir1xor1\")\n",
    "# Load irXxorY\n",
    "# dataset = load_dataset(\"ASSERT-KTH/repairllama-dataset\", \"irXxorY\")\n",
    "\n",
    "def add_question(example):\n",
    "    \"\"\" Add a new feature- question to the dataset \"\"\"\n",
    "    if 'question' not in example:\n",
    "        example['question'] = 'What is the fix version of the code for the following vulnerability?'\n",
    "    return example\n",
    "\n",
    "def prepare_examples(dataset):\n",
    "    \"\"\" Similarize the dataset by adding a question to the dataset  and renaming the columns\"\"\"\n",
    "    dataset = dataset.map(add_question)\n",
    "    # rename the columns\n",
    "    dataset = dataset.rename_column('input', 'vulnerable')\n",
    "    dataset = dataset.rename_column('output', 'fix')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = prepare_examples(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tpublic synchronized void initComponentActivity(Body body) {\n",
      "\t\tif (!init) {\n",
      "\t\t\tthis.logger = LoggerFactory.getLogger(this.getClass());\n",
      "\t\t\tlogger.info(\"Initialising {} component.\", this.getClass().getSimpleName());\n",
      "\t\t\teleGenerator = new EleGeneratorForConstructQuery();\n",
      "\t\t\ttry {\n",
      "\t\t\t\tString address = Constants.getProperties().getProperty(\"platfomservices.querydispatchapi.endpoint\");\n",
      "\t\t\t\tsoapServer = Endpoint.publish(address, this);\n",
      "\t\t\t\tlogger.info(\"QueryDispatch SOAP service started at {}.\", address);\n",
      "\t\t\t} catch (Exception e) {\n",
      "\t\t\t\tlogger.error(\"Exception while publishing QueryDispatch SOAP Service\", e);\n",
      "\t\t\t}\n",
      "\t\t\ttry {\n",
      "\t\t\t\trestServer = new PlayPlatformservicesRest(this);\n",
      "\t        \tlogger.info(String.format(\"QueryDispatch REST service started at %s with WADL remotely available at \"\n",
      "\t        \t\t\t+ \"%s/application.wadl\\n\", PlayPlatformservicesRest.BASE_URI, Constants.getProperties().getProperty(\"platfomservices.querydispatchapi.rest\")));\n",
      "\t\t\t} catch (Exception e) {\n",
      "\t\t\t\tlogger.error(\"Exception while publishing QueryDispatch REST Service\", e);\n",
      "\t\t\t}\n",
      "\t\t\tthis.init = true;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "    public void testEncodeDecode() {\n",
      "        byte[] bytes = new byte[256];\n",
      "        new Random(1L).nextBytes(bytes);\n",
      "        final String encodedBytes = HexDecoder.encode(bytes);\n",
      "        assertEquals(\n",
      "                \"Bad encoded data\",\n",
      "                \"73D51ABBD89CB8196F0EFB6892F94D68FCCC2C35F0B84609E5F12C55DD85ABA8D5D9BEF76808F3B572E5900112B81927BA5BB5F67E1BDA28B4049BF0E4AED78DB15D7BF2FC0C34E9A99DE4EF3BC2B17C8137AD659878F9E93DF1F658367ACA286452474B9EF3765E24E9A88173724DDDFB04B01DCCEB0C8AEAD641C58DAD569581BAEEA87C10D40A47902028E61CFDC243D9D16008AABC9FB77CC723A56017E14F1CE8B1698341734A6823CE02043E016B544901214A2DDAB82FEC85C0B9FE0549C475BE5B887BB4B8995B24FB5C6846F88B527B4F9D4C1391F1678B23BA4F9C9CD7BC93EB5776F4F03675344864294661C5949FAF17B130FCF6482F971A5500\",\n",
      "                encodedBytes);\n",
      "        final byte[] decodedBytes = HexDecoder.decode(encodedBytes);\n",
      "        assertTrue(\"Bad decoded bytes\", Arrays.equals(bytes, decodedBytes));\n",
      "    }\n",
      "\n",
      "    public String getGeoSearchSql(final Object domainKey) {\n",
      "        final String sql = \"\"\n",
      "                    + \"SELECT DISTINCT i.class_id , \"\n",
      "                    + \"                i.object_id, \"\n",
      "                    + \"                s.stringrep \"\n",
      "                    + \"FROM            geom g, \"\n",
      "                    + \"                cs_attr_object_derived i \"\n",
      "                    + \"                LEFT OUTER JOIN cs_stringrepcache s \"\n",
      "                    + \"                ON              ( \"\n",
      "                    + \"                                                s.class_id =i.class_id \"\n",
      "                    + \"                                AND             s.object_id=i.object_id \"\n",
      "                    + \"                                ) \"\n",
      "                    + \"WHERE           i.attr_class_id = \"\n",
      "                    + \"                ( SELECT cs_class.id \"\n",
      "                    + \"                FROM    cs_class \"\n",
      "                    + \"                WHERE   cs_class.table_name::text = 'GEOM'::text \"\n",
      "                    + \"                ) \"\n",
      "                    + \"AND             i.attr_object_id = g.id \"\n",
      "                    + \"AND i.class_id IN <cidsClassesInStatement> \"\n",
      "                    + \"AND geo_field && GeometryFromText('SRID=<cidsSearchGeometrySRID>;<cidsSearchGeometryWKT>') \"\n",
      "                    + \"AND <intersectsStatement> \"\n",
      "                    + \"ORDER BY        1,2,3\";\n",
      "        final String sqlAlt = \"\"\n",
      "                    + \"\\nWITH recursive derived_index(ocid,oid,acid,aid,depth) AS \"\n",
      "                    + \"\\n( SELECT class_id, \"\n",
      "                    + \"\\n        object_id, \"\n",
      "                    + \"\\n        class_id , \"\n",
      "                    + \"\\n        object_id, \"\n",
      "                    + \"\\n        0 \"\n",
      "                    + \"\\nFROM    geosuche2 \"\n",
      "                    + \"\\nWHERE   class_id IN( WITH recursive derived_child(father,child,depth) AS \"\n",
      "                    + \"\\n                    ( SELECT father, \"\n",
      "                    + \"\\n                            father , \"\n",
      "                    + \"\\n                            0 \"\n",
      "                    + \"\\n                    FROM    cs_class_hierarchy \"\n",
      "                    + \"\\n                    WHERE   father IN <cidsClassesInStatement> \"\n",
      "                    + \"\\n                     \"\n",
      "                    + \"\\n                    UNION ALL \"\n",
      "                    + \"\\n                     \"\n",
      "                    + \"\\n                    SELECT ch.father, \"\n",
      "                    + \"\\n                           ch.child , \"\n",
      "                    + \"\\n                           dc.depth+1 \"\n",
      "                    + \"\\n                    FROM   derived_child dc, \"\n",
      "                    + \"\\n                           cs_class_hierarchy ch \"\n",
      "                    + \"\\n                    WHERE  ch.father=dc.child \"\n",
      "                    + \"\\n                    ) \"\n",
      "                    + \"\\n             SELECT DISTINCT child \"\n",
      "                    + \"\\n             FROM            derived_child LIMIT 100 ) \"\n",
      "                    + \"\\nAND             geo_field && GeometryFromText('SRID=<cidsSearchGeometrySRID>;<cidsSearchGeometryWKT>') \"\n",
      "                    + \"\\nAND             intersects(geo_field,GeometryFromText('SRID=<cidsSearchGeometrySRID>;<cidsSearchGeometryWKT>')) \"\n",
      "                    + \"\\n \"\n",
      "                    + \"\\nUNION ALL \"\n",
      "                    + \"\\n \"\n",
      "                    + \"\\nSELECT aam.class_id      , \"\n",
      "                    + \"\\n       aam.object_id     , \"\n",
      "                    + \"\\n       aam.attr_class_id , \"\n",
      "                    + \"\\n       aam.attr_object_id, \"\n",
      "                    + \"\\n       di.depth+1 \"\n",
      "                    + \"\\nFROM   cs_attr_object aam, \"\n",
      "                    + \"\\n       derived_index di \"\n",
      "                    + \"\\nWHERE  aam.attr_class_id =di.ocid \"\n",
      "                    + \"\\nAND    aam.attr_object_id=di.oid \"\n",
      "                    + \"\\n) \"\n",
      "                    + \"\\nSELECT DISTINCT ocid, \"\n",
      "                    + \"\\n                oid , \"\n",
      "                    + \"\\n                stringrep \"\n",
      "                    + \"\\nFROM            derived_index \"\n",
      "                    + \"\\n                LEFT OUTER JOIN cs_stringrepcache \"\n",
      "                    + \"\\n                ON              ( \"\n",
      "                    + \"\\n                                                ocid=class_id \"\n",
      "                    + \"\\n                                AND             oid =object_id \"\n",
      "                    + \"\\n                                ) \"\n",
      "                    + \"\\nWHERE           ocid IN <cidsClassesInStatement>  LIMIT 10000000 \";\n",
      "        final String intersectsStatement;\n",
      "        if (searchGeometry.getSRID() == 4326) {\n",
      "            intersectsStatement =\n",
      "                \"intersects(geo_field,GeometryFromText('SRID=<cidsSearchGeometrySRID>;<cidsSearchGeometryWKT>'))\";\n",
      "        } else {\n",
      "            if ((searchGeometry instanceof Polygon) || (searchGeometry instanceof MultiPolygon)) {    \n",
      "                intersectsStatement =\n",
      "                    \"intersects(st_buffer(geo_field, 0.000001),st_buffer(GeometryFromText('SRID=<cidsSearchGeometrySRID>;<cidsSearchGeometryWKT>'), 0.000001))\";\n",
      "            } else {                                                                                  \n",
      "                intersectsStatement =\n",
      "                    \"intersects(st_buffer(geo_field, 0.000001),GeometryFromText('SRID=<cidsSearchGeometrySRID>;<cidsSearchGeometryWKT>'))\";\n",
      "            }\n",
      "        }\n",
      "        final String cidsSearchGeometryWKT = searchGeometry.toText();\n",
      "        final String sridString = Integer.toString(searchGeometry.getSRID());\n",
      "        final String classesInStatement = getClassesInSnippetsPerDomain().get((String)domainKey);\n",
      "        if ((cidsSearchGeometryWKT == null) || (cidsSearchGeometryWKT.trim().length() == 0)\n",
      "                    || (sridString == null)\n",
      "                    || (sridString.trim().length() == 0)) {\n",
      "            getLog().error(\n",
      "                \"Search geometry or srid is not given. Can't perform a search without those information.\");\n",
      "            return null;\n",
      "        }\n",
      "        if (getLog().isDebugEnabled()) {\n",
      "            getLog().debug(\"cidsClassesInStatement=\" + classesInStatement);\n",
      "        }\n",
      "        if (getLog().isDebugEnabled()) {\n",
      "            getLog().debug(\"cidsSearchGeometryWKT=\" + cidsSearchGeometryWKT);\n",
      "        }\n",
      "        if (getLog().isDebugEnabled()) {\n",
      "            getLog().debug(\"cidsSearchGeometrySRID=\" + sridString);\n",
      "        }\n",
      "        if (getLog().isDebugEnabled()) {\n",
      "            getLog().debug(\"intersectsStatement=\" + intersectsStatement);\n",
      "        }\n",
      "        if ((classesInStatement == null) || (classesInStatement.trim().length() == 0)) {\n",
      "            getLog().warn(\"There are no search classes defined for domain '\" + domainKey\n",
      "                        + \"'. This domain will be skipped.\");\n",
      "            return null;\n",
      "        }\n",
      "        return sql.replaceAll(\"<intersectsStatement>\", intersectsStatement)\n",
      "                    .replaceAll(\"<cidsClassesInStatement>\", classesInStatement)\n",
      "                    .replaceAll(\"<cidsSearchGeometryWKT>\", cidsSearchGeometryWKT)\n",
      "                    .replaceAll(\"<cidsSearchGeometrySRID>\", sridString);\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vul, lang in zip(dataset['test'][:3]['vulnerable'], dataset['test'][:3]['fix']):\n",
    "    # print(x)\n",
    "    print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_precision(model):\n",
    "    param_precisions = set()\n",
    "    buffer_precisions = set()\n",
    "    \n",
    "    # Check parameter precisions\n",
    "    for param in model.parameters():\n",
    "        param_precisions.add(param.dtype)\n",
    "    \n",
    "    # Check buffer precisions\n",
    "    for buffer in model.buffers():\n",
    "        buffer_precisions.add(buffer.dtype)\n",
    "    \n",
    "    print(\"Parameter precisions:\", param_precisions)\n",
    "    print(\"Buffer precisions:\", buffer_precisions)\n",
    "    # return param_precisions, buffer_precisions\n",
    "\n",
    "def is_model_quantized(model):\n",
    "    return any(param.dtype == torch.qint8 for param in model.parameters())\n",
    "    # return any(param.dtype == torch.qint4 for param in model.parameters())\n",
    "\n",
    "\n",
    "# NOTE:  Since PyTorch does not have a native 4-bit floating-point or integer data type, \n",
    "# libraries like bitsandbytes handle 4-bit quantization internally. \n",
    "# Check the model's precision levels\n",
    "\n",
    "check_model_precision(model)\n",
    "is_model_quantized(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from transformers import GenerationConfig\n",
    "from codebleu import calc_codebleu\n",
    "from tabulate import tabulate\n",
    "from logging import getLogger\n",
    "from configparser import ConfigParser\n",
    "\n",
    "dash_line = \"-\" * 100\n",
    "\n",
    "\n",
    "log = getLogger(__name__)\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    # Tokenize and move to device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
    "\n",
    "    # print('generating..')\n",
    "\n",
    "    # Generate text\n",
    "    model_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=512,\n",
    "            # Optional: tweak other parameters for speed\n",
    "            do_sample=True,  # sampling instead of greedy decoding\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        ),\n",
    "    )\n",
    "    # print('decoding...')\n",
    "    # Decode the generated text\n",
    "    text_output = tokenizer.decode(\n",
    "        model_output[0], skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return text_output\n",
    "\n",
    "def generate_fixes(\n",
    "    original_model,\n",
    "    instruct_model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    result_csv,\n",
    "):\n",
    "    \"\"\"\" Generate fixes for a list of vulnerables using a model \"\"\"\n",
    "    human_baseline_fixes = dataset['test']['fix'][0:4]\n",
    "    programming_languages = len(human_baseline_fixes) * ['Java']\n",
    "    original_model_fixes = []\n",
    "    instruct_model_fixes = []\n",
    "\n",
    "    for _, vulnerable in enumerate(dataset['test']['vulnerable'][0:4]):\n",
    "        prompt = f\"\"\"\n",
    "                    Generation the fix for the following vulnerable code:\n",
    "                    Vulnerable:\n",
    "\n",
    "                    {vulnerable}\n",
    "\n",
    "                    fix: \\n\"\"\"\n",
    "        # print(prompt)\n",
    "    \n",
    "        original_model_text_output = generate_text(original_model, tokenizer, prompt)\n",
    "        # print(original_model_text_output)\n",
    "        # print(dash_line)   \n",
    "\n",
    "        original_model_fixes.append(original_model_text_output)\n",
    "\n",
    "\n",
    "        instruct_model_text_output = generate_text(original_model, tokenizer, prompt)\n",
    "        # print(instruct_model_text_output)\n",
    "        # print(dash_line)\n",
    "\n",
    "        instruct_model_fixes.append(instruct_model_text_output)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        zip(\n",
    "            human_baseline_fixes,\n",
    "            original_model_fixes,\n",
    "            instruct_model_fixes,\n",
    "            programming_languages,\n",
    "        ),\n",
    "        columns=[\n",
    "            \"human_baseline_fixes\",\n",
    "            \"original_model_fixes\",\n",
    "            \"instruct_model_fixes\",\n",
    "            \"programming_language\",\n",
    "        ],\n",
    "    )\n",
    "    df.to_csv(result_csv, index=False)\n",
    "    log.info(dash_line)\n",
    "    log.info(f\"Results of vul-fix-training saved to {result_csv}\")\n",
    "    log.info(dash_line)\n",
    "    log.info(\"Sample of the results:\")\n",
    "    log.info(df.head())\n",
    "    log.info(dash_line)\n",
    "    return df\n",
    "\n",
    "# generate_fixes(\n",
    "#     model,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     dataset,\n",
    "#     'results.csv',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "            Generation the fix for the following vulnerable C code, the vulnerable code is a division by zero error:\n",
    "            Vulnerable:\n",
    "\n",
    "            public class Test {\n",
    "                public static void main(String[] args) {\n",
    "                    int a = 10;\n",
    "                    int b = 0;\n",
    "                    int c = a / b;\n",
    "                    System.out.println(c);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            fix: \\n\n",
    "        \"\"\"\n",
    "\n",
    "print(generate_text(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "output_dir = \"models/Model-7b-quantized-4bit\"\n",
    "# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     # load_in_8bit=True,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# To load a fine-tuned Lora/Qlora adapter use PeftModel.from_pretrained.\n",
    "# output_dir should be something containing an adapter_config.json and adapter_model.bin:\n",
    "model = PeftModel.from_pretrained(model, output_dir)\n",
    "\n",
    "\n",
    "# # 8. Evaluate the model\n",
    "# eval_prompt = generate_eval_prompt(dataset[\"test\"][0])\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input,\n",
    "#           max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"You are a powerful code-fixing model. Your job is to analyze and fix vulnerabilities in code. You are given a snippet of vulnerable code and its context.\n",
    "\n",
    "You must output the fixed version of the code snippet.\n",
    "\n",
    "### Input:\n",
    "{data_point[\"question\"]}\n",
    "\n",
    "### Context:\n",
    "{data_point[\"context\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"answer\"]}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat to prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "dataset['test'].map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # put model back into training mode\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_response(model, tokenizer, device, chat, max_new_tokens=200):\n",
    "\n",
    "   inputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(device)\n",
    "   output = model.generate(input_ids=inputs, max_new_tokens=max_new_tokens)\n",
    "   output = output[0].to(device)\n",
    "   return tokenizer.decode(output)\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n",
    "   {\"role\": \"user\", \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n",
    "]\n",
    "response = generate_chat_response(model, tokenizer, device, chat)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RepairLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vul = dataset['test'][0]['vulnerable']\n",
    "patch = dataset['test'][0]['fix']\n",
    "\n",
    "prompt = f\"Fix the vulnerability in the following code:\\n{vul}\\n\\nPatch:\\n{patch}\"\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in Python. Please, provide all answers to programming questions in C\"},\n",
    "   {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "response = generate_chat_response(model, tokenizer, device, chat, 200)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CodeLLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['vulnerable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "model.train() # put model back into training mode\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset for CodeLLama fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=100,\n",
    "        learning_rate=float(config['fine_tuning']['learning_rate']),\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        save_strategy=\"no\",\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "        model, type(model)\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    trainer.save_model(output_dir)\n",
    "    log.info(\"Fine-Tuning Completed!\")\n",
    "    log.info(\"Model saved to:\", output_dir)\n",
    "    log.info(\"=\" * 50)\n",
    "    return trainer, model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
