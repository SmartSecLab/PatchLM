{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering using CodeLlamma API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Assuming you're using CUDA, set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f'Available device: {device}')\n",
    "\n",
    "\n",
    "# most lightweight model of CodeLlama for instruction prompt\n",
    "# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "# base_model = 'QuantFactory/CodeLlama-7b-hf-GGUF'\n",
    "# model_id = '/Users/guru/research/LLMs/CodeLlama-70-Instruct-hf'\n",
    "base_model = 'models/CodeLLama-7b-quantized-4bit'\n",
    "output_dir = 'models/patch-code-llama/checkpoint-400'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose between 4-bit or 8-bit quantization\n",
    "use_4bit = True  # Set to False for 8-bit quantization\n",
    "\n",
    "# Configure quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,  \n",
    "    # load_in_8bit_fp32_cpu_offload=not use_4bit\n",
    ")\n",
    "\n",
    "# Initialize the model with empty weights\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        quantization_config=quantization_config,\n",
    "        device_map='auto',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_4bit:\n",
    "    model.save_pretrained('models/Model-7b-quantized-4bit')\n",
    "    tokenizer.save_pretrained('models/Model-7b-quantized-4bit')\n",
    "else:\n",
    "    model.save_pretrained('models/Model-7b-quantized-8bit')\n",
    "    tokenizer.save_pretrained('models/Model-7b-quantized-8bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_model= AutoModelForCausalLM.from_pretrained(\n",
    "#     'models/CodeLLama-Debug', \n",
    "#     quantization_config=quantization_config, \n",
    "#     device_map='auto'\n",
    "#     )\n",
    "# model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "instruct_model = 'models/CodeLLama-Debug'\n",
    "\n",
    "\n",
    "\n",
    "instruct_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=instruct_model,\n",
    "    device_map='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load ir1xor1\n",
    "dataset = load_dataset(\"ASSERT-KTH/repairllama-datasets\", \"ir1xor1\")\n",
    "# Load irXxorY\n",
    "# dataset = load_dataset(\"ASSERT-KTH/repairllama-dataset\", \"irXxorY\")\n",
    "\n",
    "def add_question(example):\n",
    "    \"\"\" Add a new feature- question to the dataset \"\"\"\n",
    "    if 'question' not in example:\n",
    "        example['question'] = 'What is the fix version of the code for the following vulnerability?'\n",
    "    return example\n",
    "\n",
    "def prepare_examples(dataset):\n",
    "    \"\"\" Similarize the dataset by adding a question to the dataset  and renaming the columns\"\"\"\n",
    "    dataset = dataset.map(add_question)\n",
    "    # rename the columns\n",
    "    dataset = dataset.rename_column('input', 'vulnerable')\n",
    "    dataset = dataset.rename_column('output', 'fix')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = prepare_examples(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_precision(model):\n",
    "    param_precisions = set()\n",
    "    buffer_precisions = set()\n",
    "    \n",
    "    # Check parameter precisions\n",
    "    for param in model.parameters():\n",
    "        param_precisions.add(param.dtype)\n",
    "    \n",
    "    # Check buffer precisions\n",
    "    for buffer in model.buffers():\n",
    "        buffer_precisions.add(buffer.dtype)\n",
    "    \n",
    "    print(\"Parameter precisions:\", param_precisions)\n",
    "    print(\"Buffer precisions:\", buffer_precisions)\n",
    "    # return param_precisions, buffer_precisions\n",
    "\n",
    "def is_model_quantized(model):\n",
    "    return any(param.dtype == torch.qint8 for param in model.parameters())\n",
    "    # return any(param.dtype == torch.qint4 for param in model.parameters())\n",
    "\n",
    "\n",
    "# NOTE:  Since PyTorch does not have a native 4-bit floating-point or integer data type, \n",
    "# libraries like bitsandbytes handle 4-bit quantization internally. \n",
    "# Check the model's precision levels\n",
    "\n",
    "check_model_precision(model)\n",
    "is_model_quantized(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from transformers import GenerationConfig\n",
    "from codebleu import calc_codebleu\n",
    "from tabulate import tabulate\n",
    "from logging import getLogger\n",
    "from configparser import ConfigParser\n",
    "\n",
    "dash_line = \"-\" * 100\n",
    "\n",
    "\n",
    "log = getLogger(__name__)\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    # Tokenize and move to device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
    "\n",
    "    # print('generating..')\n",
    "\n",
    "    # Generate text\n",
    "    model_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=512,\n",
    "            # Optional: tweak other parameters for speed\n",
    "            do_sample=True,  # sampling instead of greedy decoding\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        ),\n",
    "    )\n",
    "    # print('decoding...')\n",
    "    # Decode the generated text\n",
    "    text_output = tokenizer.decode(\n",
    "        model_output[0], skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return text_output\n",
    "\n",
    "def generate_fixes(\n",
    "    original_model,\n",
    "    instruct_model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    result_csv,\n",
    "):\n",
    "    \"\"\"\" Generate fixes for a list of vulnerables using a model \"\"\"\n",
    "    human_baseline_fixes = dataset['test']['fix'][0:4]\n",
    "    programming_languages = len(human_baseline_fixes) * ['Java']\n",
    "    original_model_fixes = []\n",
    "    instruct_model_fixes = []\n",
    "\n",
    "    for _, vulnerable in enumerate(dataset['test']['vulnerable'][0:4]):\n",
    "        prompt = f\"\"\"\n",
    "                    Generation the fix for the following vulnerable code:\n",
    "                    Vulnerable:\n",
    "\n",
    "                    {vulnerable}\n",
    "\n",
    "                    fix: \\n\"\"\"\n",
    "        # print(prompt)\n",
    "    \n",
    "        original_model_text_output = generate_text(original_model, tokenizer, prompt)\n",
    "        # print(original_model_text_output)\n",
    "        # print(dash_line)   \n",
    "\n",
    "        original_model_fixes.append(original_model_text_output)\n",
    "\n",
    "\n",
    "        instruct_model_text_output = generate_text(original_model, tokenizer, prompt)\n",
    "        # print(instruct_model_text_output)\n",
    "        # print(dash_line)\n",
    "\n",
    "        instruct_model_fixes.append(instruct_model_text_output)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        zip(\n",
    "            human_baseline_fixes,\n",
    "            original_model_fixes,\n",
    "            instruct_model_fixes,\n",
    "            programming_languages,\n",
    "        ),\n",
    "        columns=[\n",
    "            \"human_baseline_fixes\",\n",
    "            \"original_model_fixes\",\n",
    "            \"instruct_model_fixes\",\n",
    "            \"programming_language\",\n",
    "        ],\n",
    "    )\n",
    "    df.to_csv(result_csv, index=False)\n",
    "    log.info(dash_line)\n",
    "    log.info(f\"Results of vul-fix-training saved to {result_csv}\")\n",
    "    log.info(dash_line)\n",
    "    log.info(\"Sample of the results:\")\n",
    "    log.info(df.head())\n",
    "    log.info(dash_line)\n",
    "    return df\n",
    "\n",
    "# generate_fixes(\n",
    "#     model,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     dataset,\n",
    "#     'results.csv',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "            Generation the fix for the following vulnerable C code, the vulnerable code is a division by zero error:\n",
    "            Vulnerable:\n",
    "\n",
    "            public class Test {\n",
    "                public static void main(String[] args) {\n",
    "                    int a = 10;\n",
    "                    int b = 0;\n",
    "                    int c = a / b;\n",
    "                    System.out.println(c);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            fix: \\n\n",
    "        \"\"\"\n",
    "\n",
    "print(generate_text(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "output_dir = \"models/Model-7b-quantized-4bit\"\n",
    "# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     # load_in_8bit=True,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# To load a fine-tuned Lora/Qlora adapter use PeftModel.from_pretrained.\n",
    "# output_dir should be something containing an adapter_config.json and adapter_model.bin:\n",
    "model = PeftModel.from_pretrained(model, output_dir)\n",
    "\n",
    "\n",
    "# # 8. Evaluate the model\n",
    "# eval_prompt = generate_eval_prompt(dataset[\"test\"][0])\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input,\n",
    "#           max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"You are a powerful code-fixing model. Your job is to analyze and fix vulnerabilities in code. You are given a snippet of vulnerable code and its context.\n",
    "\n",
    "You must output the fixed version of the code snippet.\n",
    "\n",
    "### Input:\n",
    "{data_point[\"question\"]}\n",
    "\n",
    "### Context:\n",
    "{data_point[\"context\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"answer\"]}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat to prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "dataset['test'].map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # put model back into training mode\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_response(model, tokenizer, device, chat, max_new_tokens=200):\n",
    "\n",
    "   inputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(device)\n",
    "   output = model.generate(input_ids=inputs, max_new_tokens=max_new_tokens)\n",
    "   output = output[0].to(device)\n",
    "   return tokenizer.decode(output)\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n",
    "   {\"role\": \"user\", \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n",
    "]\n",
    "response = generate_chat_response(model, tokenizer, device, chat)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RepairLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vul = dataset['test'][0]['vulnerable']\n",
    "patch = dataset['test'][0]['fix']\n",
    "\n",
    "prompt = f\"Fix the vulnerability in the following code:\\n{vul}\\n\\nPatch:\\n{patch}\"\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in Python. Please, provide all answers to programming questions in C\"},\n",
    "   {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "response = generate_chat_response(model, tokenizer, device, chat, 200)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CodeLLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['vulnerable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "model.train() # put model back into training mode\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset for CodeLLama fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=100,\n",
    "        learning_rate=float(config['fine_tuning']['learning_rate']),\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        save_strategy=\"no\",\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "        model, type(model)\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    trainer.save_model(output_dir)\n",
    "    log.info(\"Fine-Tuning Completed!\")\n",
    "    log.info(\"Model saved to:\", output_dir)\n",
    "    log.info(\"=\" * 50)\n",
    "    return trainer, model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
