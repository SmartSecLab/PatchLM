{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering using CodeLlamma API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Assuming you're using CUDA, set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f'Available device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/miniconda3/envs/patch/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374cd4fbb602406e8f2539a4dc5f626f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97aca2b128ac4bd980a268d84e894093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1809505ea6b946d19f978eeee1e4e358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73a7154140c452495ff0e8e5e273037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91b18be8c614ddb9365f50a7f09a29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501a44e24fb049ba942dfd5e5431d9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3e93dfd23a48d491ed32ecf1fbf95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/miniconda3/envs/patch/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# most lightweight model of CodeLlama for instruction prompt\n",
    "model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# model_id = '/Users/guru/research/LLMs/CodeLlama-70-Instruct-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, force_download=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   model_id,\n",
    "   torch_dtype=torch.float16,\n",
    "   device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      "You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\n",
      "<</SYS>>\n",
      "\n",
      "Write a function that computes the set of sums of all contiguous sublists of a given list. [/INST]  Here is a possible implementation of the function you described:\n",
      "```\n",
      "function computeSums(list) {\n",
      "  let sums = [];\n",
      "  for (let i = 0; i < list.length; i++) {\n",
      "    let sum = 0;\n",
      "    for (let j = i; j < list.length; j++) {\n",
      "      sum += list[j];\n",
      "    }\n",
      "    sums.push(sum);\n",
      "  }\n",
      "  return sums;\n",
      "}\n",
      "```\n",
      "This function takes a list as input and returns a list of the sums of all contiguous sublists of the input list.\n",
      "\n",
      "For example, if the input list is `[1, 2, 3, 4, 5]`, the output list would be `[15, 14, 12, 7, 0]`.\n",
      "\n",
      "Here's an explanation of how the function works:\n",
      "\n",
      "1. We initialize\n"
     ]
    }
   ],
   "source": [
    "def generate_chat_response(model, tokenizer, device, chat, max_new_tokens=200):\n",
    "\n",
    "   inputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(device)\n",
    "   output = model.generate(input_ids=inputs, max_new_tokens=max_new_tokens)\n",
    "   output = output[0].to(device)\n",
    "   return tokenizer.decode(output)\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n",
    "   {\"role\": \"user\", \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n",
    "]\n",
    "response = generate_chat_response(model, tokenizer, device, chat)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RepairLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load ir1xor1\n",
    "dataset = load_dataset(\"ASSERT-KTH/repairllama-datasets\", \"ir1xor1\")\n",
    "# Load irXxorY\n",
    "# dataset = load_dataset(\"ASSERT-KTH/repairllama-dataset\", \"irXxorY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "vul = dataset['test'][0]['input']\n",
    "patch = dataset['test'][0]['output']\n",
    "\n",
    "prompt = f\"Fix the vulnerability in the following code:\\n{vul}\\n\\nPatch:\\n{patch}\"\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in Python. Please, provide all answers to programming questions in C\"},\n",
    "   {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "response = generate_chat_response(model, tokenizer, device, chat, 200)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"Fix the vulnerability in the following code:\\n{vul}\\n\\nWhat is the patch?\"\n",
    "\n",
    "\n",
    "# chat = [\n",
    "#    {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in Python. Please, provide all answers to programming questions in C\"},\n",
    "#    {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# response = generate_chat_response(model, tokenizer, device, chat, 200)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.half()\n",
    "# model.to(device)\n",
    "\n",
    "# response = generate_chat_response(model, tokenizer, device, chat, 500)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model\n",
    "# model = AutoModelForCausalLM.from_pretrained('models/CodeLlama-7b-Instruct-hf')\n",
    "\n",
    "# response = generate_chat_response(model, tokenizer, device, chat, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CodeLLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
      "\n",
      "You must output the SQL query that answers the question.\n",
      "### Input:\n",
      "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
      "\n",
      "### Response:\n",
      "SELECT * FROM table_name_12 WHERE frequency_mhz > 91.5 AND city_of_license = 'hyannis';\n",
      "\n",
      "### Input:\n",
      "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "### Input:\n",
    "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
    "\n",
    "### Context:\n",
    "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "# {'question': 'Name the comptroller for office of prohibition', 'context': 'CREATE TABLE table_22607062_1 (comptroller VARCHAR, ticket___office VARCHAR)', 'answer': 'SELECT comptroller FROM table_22607062_1 WHERE ticket___office = \"Prohibition\"'}\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    public static void beforeClass() throws Exception {\\n        server = new DummyServer(\"localhost\", 6999);\\n        server.start();\\n        server.register(\"my-app\", \"my-module\", null, EchoBean.class.getSimpleName(), new EchoBean());\\n        final Endpoint endpoint = Remoting.createEndpoint(\"endpoint\", Executors.newSingleThreadExecutor(), OptionMap.EMPTY);\\n        final Xnio xnio = Xnio.getInstance();\\n        final Registration registration = endpoint.addConnectionProvider(\"remote\", new RemoteConnectionProviderFactory(xnio), OptionMap.create(Options.SSL_ENABLED, false));\\n        final IoFuture<Connection> futureConnection = endpoint.connect(new URI(\"remote://localhost:6999\"), OptionMap.create(Options.SASL_POLICY_NOANONYMOUS, Boolean.FALSE), new AnonymousCallbackHandler());\\n        connection = get(futureConnection, 5, TimeUnit.SECONDS);\\n    }\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "\n",
    "### Input:\n",
    "What is the patch for the following vulnerability?\n",
    "\n",
    "### Context:\n",
    "{dataset['train'][0]['input']}\n",
    "\n",
    "### Response:\n",
    "{dataset['train'][0]['output']}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   13108 MB |   14043 MB |  148238 MB |  135129 MB |\n",
      "|       from large pool |   13108 MB |   14042 MB |  135522 MB |  122414 MB |\n",
      "|       from small pool |       0 MB |     128 MB |   12715 MB |   12715 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   13108 MB |   14043 MB |  148238 MB |  135129 MB |\n",
      "|       from large pool |   13108 MB |   14042 MB |  135522 MB |  122414 MB |\n",
      "|       from small pool |       0 MB |     128 MB |   12715 MB |   12715 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   14460 MB |   14460 MB |   14462 MB |    2048 KB |\n",
      "|       from large pool |   14326 MB |   14326 MB |   14326 MB |       0 KB |\n",
      "|       from small pool |     134 MB |     134 MB |     136 MB |    2048 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    9446 KB |  436821 KB |  170802 MB |  170793 MB |\n",
      "|       from large pool |    7936 KB |  435488 KB |  157194 MB |  157187 MB |\n",
      "|       from small pool |    1510 KB |   45250 KB |   13607 MB |   13606 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     389    |     537    |     853 K  |     853 K  |\n",
      "|       from large pool |     290    |     419    |      29 K  |      29 K  |\n",
      "|       from small pool |      99    |     247    |     823 K  |     823 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     389    |     537    |     853 K  |     853 K  |\n",
      "|       from large pool |     290    |     419    |      29 K  |      29 K  |\n",
      "|       from small pool |      99    |     247    |     823 K  |     823 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     355    |     355    |     356    |       1    |\n",
      "|       from large pool |     288    |     288    |     288    |       0    |\n",
      "|       from small pool |      67    |      67    |      68    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       4    |      80    |  387153    |  387149    |\n",
      "|       from large pool |       3    |      57    |    8739    |    8736    |\n",
      "|       from small pool |       1    |      77    |  378414    |  378413    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 15.74 GiB total capacity; 14.99 GiB already allocated; 49.81 MiB free; 15.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# put model back into training mode\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     12\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     13\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/patch/lib/python3.8/site-packages/peft/utils/other.py:126\u001b[0m, in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    124\u001b[0m             (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m    125\u001b[0m         ) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m             param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    129\u001b[0m     loaded_in_kbit \u001b[38;5;129;01mor\u001b[39;00m is_gptq_quantized \u001b[38;5;129;01mor\u001b[39;00m is_aqlm_quantized \u001b[38;5;129;01mor\u001b[39;00m is_eetq_quantized \u001b[38;5;129;01mor\u001b[39;00m is_hqq_quantized\n\u001b[1;32m    130\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m use_gradient_checkpointing:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# When having `use_reentrant=False` + gradient_checkpointing, there is no need for this hack\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gradient_checkpointing_kwargs \u001b[38;5;129;01mor\u001b[39;00m gradient_checkpointing_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 15.74 GiB total capacity; 14.99 GiB already allocated; 49.81 MiB free; 15.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "model.train() # put model back into training mode\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset for CodeLLama fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answer', 'question', 'context'],\n",
       "    num_rows: 7858\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "dataset_mc2 = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "train_dataset = dataset_mc2.train_test_split(test_size=0.1)[\"train\"]\n",
    "eval_dataset = dataset_mc2.train_test_split(test_size=0.1)[\"test\"]\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output'],\n",
       "    num_rows: 64643\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
