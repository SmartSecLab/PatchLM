{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering using CodeLlamma API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/miniconda3/envs/patch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Assuming you're using CUDA, set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print(f'Available device: {device}')\n",
    "\n",
    "\n",
    "# most lightweight model of CodeLlama for instruction prompt\n",
    "# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "# base_model = 'QuantFactory/CodeLlama-7b-hf-GGUF'\n",
    "# model_id = '/Users/guru/research/LLMs/CodeLlama-70-Instruct-hf'\n",
    "base_model = 'models/CodeLLama-7b-quantized-4bit'\n",
    "output_dir = 'models/patch-code-llama/checkpoint-400'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/guru/miniconda3/envs/patch/lib/python3.10/site-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose between 4-bit or 8-bit quantization\n",
    "use_4bit = True  # Set to False for 8-bit quantization\n",
    "\n",
    "# Configure quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,  \n",
    "    # load_in_8bit_fp32_cpu_offload=not use_4bit\n",
    ")\n",
    "\n",
    "# Initialize the model with empty weights\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        quantization_config=quantization_config,\n",
    "        device_map='auto',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/Model-7b-quantized-4bit/tokenizer_config.json',\n",
       " 'models/Model-7b-quantized-4bit/special_tokens_map.json',\n",
       " 'models/Model-7b-quantized-4bit/tokenizer.model',\n",
       " 'models/Model-7b-quantized-4bit/added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_4bit:\n",
    "    model.save_pretrained('models/Model-7b-quantized-4bit')\n",
    "    tokenizer.save_pretrained('models/Model-7b-quantized-4bit')\n",
    "else:\n",
    "    model.save_pretrained('models/Model-7b-quantized-8bit')\n",
    "    tokenizer.save_pretrained('models/Model-7b-quantized-8bit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_model= AutoModelForCausalLM.from_pretrained(\n",
    "#     'models/CodeLLama-Debug', \n",
    "#     quantization_config=quantization_config, \n",
    "#     device_map='auto'\n",
    "#     )\n",
    "# model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PeftConfigMixin.from_pretrained() missing 1 required positional argument: 'pretrained_model_name_or_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[1;32m      3\u001b[0m instruct_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/CodeLLama-Debug\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstruct_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m instruct_model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m     model_id\u001b[38;5;241m=\u001b[39minstruct_model,\n\u001b[1;32m     14\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: PeftConfigMixin.from_pretrained() missing 1 required positional argument: 'pretrained_model_name_or_path'"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "instruct_model = 'models/CodeLLama-Debug'\n",
    "\n",
    "\n",
    "\n",
    "instruct_model = PeftModel.from_pretrained(\n",
    "    model=model,\n",
    "    model_id=instruct_model,\n",
    "    device_map='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['vulnerable', 'fix', 'question'],\n",
      "        num_rows: 64643\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['vulnerable', 'fix', 'question'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load ir1xor1\n",
    "dataset = load_dataset(\"ASSERT-KTH/repairllama-datasets\", \"ir1xor1\")\n",
    "# Load irXxorY\n",
    "# dataset = load_dataset(\"ASSERT-KTH/repairllama-dataset\", \"irXxorY\")\n",
    "\n",
    "def add_question(example):\n",
    "    \"\"\" Add a new feature- question to the dataset \"\"\"\n",
    "    if 'question' not in example:\n",
    "        example['question'] = 'What is the fix version of the code for the following vulnerability?'\n",
    "    return example\n",
    "\n",
    "def prepare_examples(dataset):\n",
    "    \"\"\" Similarize the dataset by adding a question to the dataset  and renaming the columns\"\"\"\n",
    "    dataset = dataset.map(add_question)\n",
    "    # rename the columns\n",
    "    dataset = dataset.rename_column('input', 'vulnerable')\n",
    "    dataset = dataset.rename_column('output', 'fix')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = prepare_examples(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter precisions: {torch.float16, torch.uint8}\n",
      "Buffer precisions: {torch.float32}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_model_precision(model):\n",
    "    param_precisions = set()\n",
    "    buffer_precisions = set()\n",
    "    \n",
    "    # Check parameter precisions\n",
    "    for param in model.parameters():\n",
    "        param_precisions.add(param.dtype)\n",
    "    \n",
    "    # Check buffer precisions\n",
    "    for buffer in model.buffers():\n",
    "        buffer_precisions.add(buffer.dtype)\n",
    "    \n",
    "    print(\"Parameter precisions:\", param_precisions)\n",
    "    print(\"Buffer precisions:\", buffer_precisions)\n",
    "    # return param_precisions, buffer_precisions\n",
    "\n",
    "def is_model_quantized(model):\n",
    "    return any(param.dtype == torch.qint8 for param in model.parameters())\n",
    "    # return any(param.dtype == torch.qint4 for param in model.parameters())\n",
    "\n",
    "\n",
    "# NOTE:  Since PyTorch does not have a native 4-bit floating-point or integer data type, \n",
    "# libraries like bitsandbytes handle 4-bit quantization internally. \n",
    "# Check the model's precision levels\n",
    "\n",
    "check_model_precision(model)\n",
    "is_model_quantized(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from transformers import GenerationConfig\n",
    "from codebleu import calc_codebleu\n",
    "from tabulate import tabulate\n",
    "from logging import getLogger\n",
    "from configparser import ConfigParser\n",
    "\n",
    "dash_line = \"-\" * 100\n",
    "\n",
    "\n",
    "log = getLogger(__name__)\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, prompt):\n",
    "    # Tokenize and move to device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
    "\n",
    "    # print('generating..')\n",
    "\n",
    "    # Generate text\n",
    "    model_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=512,\n",
    "            # Optional: tweak other parameters for speed\n",
    "            do_sample=True,  # sampling instead of greedy decoding\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        ),\n",
    "    )\n",
    "    # print('decoding...')\n",
    "    # Decode the generated text\n",
    "    text_output = tokenizer.decode(\n",
    "        model_output[0], skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return text_output\n",
    "\n",
    "def generate_fixes(\n",
    "    original_model,\n",
    "    instruct_model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    result_csv,\n",
    "):\n",
    "    \"\"\"\" Generate fixes for a list of vulnerables using a model \"\"\"\n",
    "    human_baseline_fixes = dataset['test']['fix'][0:4]\n",
    "    programming_languages = len(human_baseline_fixes) * ['Java']\n",
    "    original_model_fixes = []\n",
    "    instruct_model_fixes = []\n",
    "\n",
    "    for _, vulnerable in enumerate(dataset['test']['vulnerable'][0:4]):\n",
    "        prompt = f\"\"\"\n",
    "                    Generation the fix for the following vulnerable code:\n",
    "                    Vulnerable:\n",
    "\n",
    "                    {vulnerable}\n",
    "\n",
    "                    fix: \\n\"\"\"\n",
    "        # print(prompt)\n",
    "    \n",
    "        original_model_text_output = generate_text(original_model, tokenizer, prompt)\n",
    "        # print(original_model_text_output)\n",
    "        # print(dash_line)   \n",
    "\n",
    "        original_model_fixes.append(original_model_text_output)\n",
    "\n",
    "\n",
    "        instruct_model_text_output = generate_text(original_model, tokenizer, prompt)\n",
    "        # print(instruct_model_text_output)\n",
    "        # print(dash_line)\n",
    "\n",
    "        instruct_model_fixes.append(instruct_model_text_output)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        zip(\n",
    "            human_baseline_fixes,\n",
    "            original_model_fixes,\n",
    "            instruct_model_fixes,\n",
    "            programming_languages,\n",
    "        ),\n",
    "        columns=[\n",
    "            \"human_baseline_fixes\",\n",
    "            \"original_model_fixes\",\n",
    "            \"instruct_model_fixes\",\n",
    "            \"programming_language\",\n",
    "        ],\n",
    "    )\n",
    "    df.to_csv(result_csv, index=False)\n",
    "    log.info(dash_line)\n",
    "    log.info(f\"Results of vul-fix-training saved to {result_csv}\")\n",
    "    log.info(dash_line)\n",
    "    log.info(\"Sample of the results:\")\n",
    "    log.info(df.head())\n",
    "    log.info(dash_line)\n",
    "    return df\n",
    "\n",
    "# generate_fixes(\n",
    "#     model,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     dataset,\n",
    "#     'results.csv',\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Generation the fix for the following vulnerable C code, the vulnerable code is a division by zero error:\n",
      "            Vulnerable:\n",
      "\n",
      "            public class Test {\n",
      "                public static void main(String[] args) {\n",
      "                    int a = 10;\n",
      "                    int b = 0;\n",
      "                    int c = a / b;\n",
      "                    System.out.println(c);\n",
      "                }\n",
      "            }\n",
      "\n",
      "            fix: \n",
      "\n",
      "         */\n",
      "        int a = 10;\n",
      "        int b = 0;\n",
      "        int c = 0;\n",
      "        try {\n",
      "            c = a / b;\n",
      "        } catch (Exception e) {\n",
      "            System.out.println(\"Division by zero is not possible\");\n",
      "        }\n",
      "        System.out.println(c);\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "            Generation the fix for the following vulnerable C code, the vulnerable code is a division by zero error:\n",
    "            Vulnerable:\n",
    "\n",
    "            public class Test {\n",
    "                public static void main(String[] args) {\n",
    "                    int a = 10;\n",
    "                    int b = 0;\n",
    "                    int c = a / b;\n",
    "                    System.out.println(c);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            fix: \\n\n",
    "        \"\"\"\n",
    "\n",
    "print(generate_text(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     27\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/Model-7b-quantized-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     base_model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# To load a fine-tuned Lora/Qlora adapter use PeftModel.from_pretrained.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# output_dir should be something containing an adapter_config.json and adapter_model.bin:\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel\u001b[49m, output_dir)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# # 8. Evaluate the model\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# eval_prompt = generate_eval_prompt(dataset[\"test\"][0])\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#     print(tokenizer.decode(model.generate(**model_input,\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#           max_new_tokens=100)[0], skip_special_tokens=True))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "output_dir = \"models/Model-7b-quantized-4bit\"\n",
    "# base_model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     # load_in_8bit=True,\n",
    "#     torch_dtype=torch.float32,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# To load a fine-tuned Lora/Qlora adapter use PeftModel.from_pretrained.\n",
    "# output_dir should be something containing an adapter_config.json and adapter_model.bin:\n",
    "model = PeftModel.from_pretrained(model, output_dir)\n",
    "\n",
    "\n",
    "# # 8. Evaluate the model\n",
    "# eval_prompt = generate_eval_prompt(dataset[\"test\"][0])\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input,\n",
    "#           max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"You are a powerful code-fixing model. Your job is to analyze and fix vulnerabilities in code. You are given a snippet of vulnerable code and its context.\n",
    "\n",
    "You must output the fixed version of the code snippet.\n",
    "\n",
    "### Input:\n",
    "{data_point[\"question\"]}\n",
    "\n",
    "### Context:\n",
    "{data_point[\"context\"]}\n",
    "\n",
    "### Response:\n",
    "{data_point[\"answer\"]}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat to prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tokenized_train_dataset = dataset.map(generate_and_tokenize_prompt)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_and_tokenize_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/patch/lib/python3.10/site-packages/datasets/arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/patch/lib/python3.10/site-packages/datasets/arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    565\u001b[0m }\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/patch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3161\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3157\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3158\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3159\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3160\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3163\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/patch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3522\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3520\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3522\u001b[0m     example \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m   3523\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3524\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/patch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3421\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3420\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3421\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3423\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3424\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3425\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[40], line 29\u001b[0m, in \u001b[0;36mgenerate_and_tokenize_prompt\u001b[0;34m(data_point)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_and_tokenize_prompt\u001b[39m(data_point):\n\u001b[1;32m     21\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a powerful code-fixing model. Your job is to analyze and fix vulnerabilities in code. You are given a snippet of vulnerable code and its context.\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124mYou must output the fixed version of the code snippet.\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m### Input:\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mdata_point[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m### Context:\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdata_point\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m### Response:\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mdata_point[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenize(full_prompt)\n",
      "File \u001b[0;32m~/miniconda3/envs/patch/lib/python3.10/site-packages/datasets/formatting/formatting.py:271\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 271\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[1;32m    273\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'context'"
     ]
    }
   ],
   "source": [
    "# tokenized_train_dataset = dataset.map(generate_and_tokenize_prompt)\n",
    "dataset['test'].map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    # prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() # put model back into training mode\n",
    "# model = prepare_model_for_int8_training(model)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/guru/miniconda3/envs/patch/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/guru/miniconda3/envs/patch/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\n",
      "<</SYS>>\n",
      "\n",
      "Write a function that computes the set of sums of all contiguous sublists of a given list. [/INST]  Here is a function that computes the set of sums of all contiguous sublists of a given list:\n",
      "```\n",
      "function contiguousSums(list) {\n",
      "  let sums = [];\n",
      "  for (let i = 0; i < list.length; i++) {\n",
      "    let sum = 0;\n",
      "    for (let j = i; j < list.length; j++) {\n",
      "      sum += list[j];\n",
      "      sums.push(sum);\n",
      "    }\n",
      "  }\n",
      "  return sums;\n",
      "}\n",
      "```\n",
      "This function takes a list as input and returns a set of all the sums of contiguous sublists of the input list.\n",
      "\n",
      "For example, if the input list is `[1, 2, 3, 4, 5]`, the function will return the set `[1, 3, 6, 10, 15]`.\n",
      "\n",
      "Here's\n"
     ]
    }
   ],
   "source": [
    "def generate_chat_response(model, tokenizer, device, chat, max_new_tokens=200):\n",
    "\n",
    "   inputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(device)\n",
    "   output = model.generate(input_ids=inputs, max_new_tokens=max_new_tokens)\n",
    "   output = output[0].to(device)\n",
    "   return tokenizer.decode(output)\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in JavaScript. Please, provide all answers to programming questions in JavaScript\"},\n",
    "   {\"role\": \"user\", \"content\": \"Write a function that computes the set of sums of all contiguous sublists of a given list.\"},\n",
    "]\n",
    "response = generate_chat_response(model, tokenizer, device, chat)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RepairLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/guru/miniconda3/envs/patch/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/guru/miniconda3/envs/patch/lib/python3.10/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vul = dataset['test'][0]['vulnerable']\n",
    "patch = dataset['test'][0]['fix']\n",
    "\n",
    "prompt = f\"Fix the vulnerability in the following code:\\n{vul}\\n\\nPatch:\\n{patch}\"\n",
    "\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"You are a helpful and honest code assistant expert in Python. Please, provide all answers to programming questions in C\"},\n",
    "   {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "response = generate_chat_response(model, tokenizer, device, chat, 200)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning CodeLLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    public static void beforeClass() throws Exception {\\n        server = new DummyServer(\"localhost\", 6999);\\n        server.start();\\n        server.register(\"my-app\", \"my-module\", null, EchoBean.class.getSimpleName(), new EchoBean());\\n        final Endpoint endpoint = Remoting.createEndpoint(\"endpoint\", Executors.newSingleThreadExecutor(), OptionMap.EMPTY);\\n        final Xnio xnio = Xnio.getInstance();\\n        final Registration registration = endpoint.addConnectionProvider(\"remote\", new RemoteConnectionProviderFactory(xnio), OptionMap.create(Options.SSL_ENABLED, false));\\n        final IoFuture<Connection> futureConnection = endpoint.connect(new URI(\"remote://localhost:6999\"), OptionMap.create(Options.SASL_POLICY_NOANONYMOUS, Boolean.FALSE), new AnonymousCallbackHandler());\\n        connection = get(futureConnection, 5, TimeUnit.SECONDS);\\n    }\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['vulnerable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   4595 MiB |  13077 MiB |  20404 GiB |  20399 GiB |\n",
      "|       from large pool |   4402 MiB |  12691 MiB |  20347 GiB |  20342 GiB |\n",
      "|       from small pool |    193 MiB |    385 MiB |     57 GiB |     56 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   4595 MiB |  13077 MiB |  20404 GiB |  20399 GiB |\n",
      "|       from large pool |   4402 MiB |  12691 MiB |  20347 GiB |  20342 GiB |\n",
      "|       from small pool |    193 MiB |    385 MiB |     57 GiB |     56 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   4547 MiB |  12932 MiB |  20347 GiB |  20342 GiB |\n",
      "|       from large pool |   4354 MiB |  12547 MiB |  20290 GiB |  20286 GiB |\n",
      "|       from small pool |    193 MiB |    385 MiB |     56 GiB |     56 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   9882 MiB |  13382 MiB |  61800 MiB |  51918 MiB |\n",
      "|       from large pool |   9530 MiB |  12996 MiB |  60418 MiB |  50888 MiB |\n",
      "|       from small pool |    352 MiB |    386 MiB |   1382 MiB |   1030 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 180463 KiB |    930 MiB |   5414 GiB |   5414 GiB |\n",
      "|       from large pool | 177536 KiB |    929 MiB |   5356 GiB |   5356 GiB |\n",
      "|       from small pool |   2927 KiB |     40 MiB |     58 GiB |     58 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1029    |    2290    |    3293 K  |    3292 K  |\n",
      "|       from large pool |     323    |     971    |     899 K  |     899 K  |\n",
      "|       from small pool |     706    |    1319    |    2394 K  |    2393 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1029    |    2290    |    3293 K  |    3292 K  |\n",
      "|       from large pool |     323    |     971    |     899 K  |     899 K  |\n",
      "|       from small pool |     706    |    1319    |    2394 K  |    2393 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     525    |     698    |    2567    |    2042    |\n",
      "|       from large pool |     349    |     505    |    1876    |    1527    |\n",
      "|       from small pool |     176    |     193    |     691    |     515    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     148    |     264    |    1458 K  |    1458 K  |\n",
      "|       from large pool |      79    |     220    |     454 K  |     453 K  |\n",
      "|       from small pool |      69    |     108    |    1004 K  |    1004 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "\n",
    "model.train() # put model back into training mode\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset for CodeLLama fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_train_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m----> 3\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[43mnum_train_epochs\u001b[49m,\n\u001b[1;32m      4\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mper_device_train_batch_size,\n\u001b[1;32m      5\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39mgradient_accumulation_steps,\n\u001b[1;32m      6\u001b[0m     optim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaged_adamw_32bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     save_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      8\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      9\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfine_tuning\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     10\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     eval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     12\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     bf16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     group_by_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     logging_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdefault_data_collator,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m old_state_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_train_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=100,\n",
    "        logging_steps=100,\n",
    "        learning_rate=float(config['fine_tuning']['learning_rate']),\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        group_by_length=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        save_strategy=\"no\",\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "        model, type(model)\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    trainer.save_model(output_dir)\n",
    "    log.info(\"Fine-Tuning Completed!\")\n",
    "    log.info(\"Model saved to:\", output_dir)\n",
    "    log.info(\"=\" * 50)\n",
    "    return trainer, model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fixme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
